{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4y7_sbSIxg6E",
    "outputId": "929e3bc6-f359-4add-ce57-077422b03c56"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYklN3iMxg6M"
   },
   "source": [
    "# 1. Data Load\n",
    "### 전처리 width, height, input_channel 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2XqoXoUxg6N"
   },
   "outputs": [],
   "source": [
    "from load_data import load_data,class_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name =\"EEG\"\n",
    "trainx, testx,trainy,testy,batch_size = load_data(data_name =data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyupyd-hxg6T",
    "outputId": "dcff2c15-b58d-4c33-989a-d54de283fa66"
   },
   "outputs": [],
   "source": [
    "class_breakdown(trainy)\n",
    "class_breakdown(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROHthjPBxg6V"
   },
   "outputs": [],
   "source": [
    "hot_encoded_y_train = np.asarray(pd.get_dummies(np.asarray(trainy.flatten())))\n",
    "hot_encoded_y_test = np.asarray(pd.get_dummies(np.asarray(testy.flatten())))\n",
    "print(\"y train shape: {}\".format(hot_encoded_y_train.shape))\n",
    "print(\"y test shape: {}\".format(hot_encoded_y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "trainx =TimeSeriesScalerMeanVariance().fit_transform(trainx)\n",
    "testx =TimeSeriesScalerMeanVariance().fit_transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O6OPKVxRxg6X",
    "outputId": "8749c02f-83df-48c8-a352-86bfb8bfdcd4"
   },
   "outputs": [],
   "source": [
    "trainX = trainx[:, np.newaxis, :]\n",
    "trainY = hot_encoded_y_train\n",
    "\n",
    "validX =testx[:len(testx)//2,np.newaxis,:]\n",
    "validY= hot_encoded_y_test[:len(testx)//2]\n",
    "\n",
    "testX = testx[len(testx)//2:,np.newaxis,:]\n",
    "testY = hot_encoded_y_test[len(testx)//2:]\n",
    "\n",
    "print (\"trainX shape:{}\".format(trainX.shape))\n",
    "print (\"trainY shape:{}\".format(trainY.shape))\n",
    "print (\"validX shape:{}\".format(validX.shape))\n",
    "print (\"validY shape:{}\".format(validY.shape))\n",
    "print (\"testX shape:{}\".format(testX.shape))\n",
    "print (\"testY shape:{}\".format(testY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8JpxKigLxg6a"
   },
   "outputs": [],
   "source": [
    "height = trainX.shape[1]\n",
    "width = trainX.shape[2]\n",
    "input_channel = trainX.shape[3]\n",
    "\n",
    "print (\"height {}\".format(height))\n",
    "print (\"width {}\".format(width))\n",
    "print (\"input_channel {}\".format(input_channel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRkT92eLxg6c"
   },
   "source": [
    "# 2. Training condition 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96TudpYHxg6d"
   },
   "outputs": [],
   "source": [
    "t_loss=[]\n",
    "t_acc=[]\n",
    "\n",
    "v_loss=[]\n",
    "v_acc=[]\n",
    "\n",
    "val_freq = 1\n",
    "save_freq = 1\n",
    "num_epochs= 500\n",
    "\n",
    "std= 0.01\n",
    "l_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0CdpPI7xg6f"
   },
   "outputs": [],
   "source": [
    "train_history = pd.DataFrame(index=np.arange(0, num_epochs), columns=['epoch', 'loss', 'acc','timestamp'])\n",
    "valid_history = pd.DataFrame(index=np.arange(0, num_epochs/val_freq),columns=['epoch', 'loss', 'acc','timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HiU9eckxg6h"
   },
   "source": [
    "# 3. model structure 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNEeKqCOxg6K"
   },
   "outputs": [],
   "source": [
    "from FCN import CNN_MC_dropout, CNN_MC_dropout_last_conv_turnoff,CNN_MC_dropout_input_turnoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basic_CNN:\n",
    "    def __init__(self, std=0.01, batch_size=64,width=500, height =1, input_channel=3, nb_classes=2, l_rate =1e-6,reuse = False):\n",
    "        self.std=std\n",
    "        self.batch_size=batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.input_channel = input_channel\n",
    "        self.l_rate = l_rate\n",
    "        \n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        with tf.name_scope('Classifier'):\n",
    "            self.y = tf.placeholder(tf.float32, [None, self.nb_classes], name='y')\n",
    "            self.x = tf.placeholder(tf.float32, [None, self.height,self.width,self.input_channel], name='x')\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "            self.is_dropout = tf.placeholder(tf.bool)\n",
    "        \n",
    "        # keep prob가 1보다 작으면, 드랍아웃을 한다는 의미\n",
    "        #self.dropout_bool = True#tf.cond(self.keep_prob < 1.0, lambda: tf.constant(True), lambda: tf.constant(False))\n",
    "        \n",
    "        self.logits = self.build_model()\n",
    "\n",
    "        # Define loss and optimizer, minimize the squared error\n",
    "        self.cross_entropy =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.y, logits=self.logits)\n",
    "        self.cost =tf.reduce_mean(self.cross_entropy)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.l_rate).minimize(self.cost)\n",
    "\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.prediction,1),tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the session\n",
    "        self.session_conf = tf.ConfigProto()\n",
    "        self.session_conf.gpu_options.allow_growth = True\n",
    "        self.sess = tf.InteractiveSession(config=self.session_conf)\n",
    "        self.sess.run(init)\n",
    "\n",
    "        self.saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "    def build_model(self):\n",
    "        with tf.variable_scope('layer0'):\n",
    "            #b, h, w, c\n",
    "            self.input = self.x\n",
    "        # Convolutional Layer #1 and Pooling Layer #1\n",
    "        with tf.variable_scope('layer1'):\n",
    "            self.conv1 = tf.layers.conv2d(self.input, 64, [1,8], padding='SAME')\n",
    "            self.pooling1 = tf.layers.max_pooling2d(inputs=self.conv1, pool_size=[1, 4],padding=\"VALID\", strides=2)\n",
    "            self.batch1 = tf.layers.batch_normalization(self.pooling1)\n",
    "            self.relu1 = tf.nn.relu(self.batch1)\n",
    "            self.dropout1 = tf.nn.dropout(self.relu1, self.keep_prob)\n",
    "\n",
    "        # Convolutional Layer #1 and Pooling Layer #2\n",
    "        with tf.variable_scope('layer2'):\n",
    "            self.conv2 = tf.layers.conv2d(self.dropout1, 128, [1,5], padding='SAME')\n",
    "            self.pooling2 = tf.layers.max_pooling2d(inputs=self.conv2, pool_size=[1, 4] ,padding=\"VALID\", strides=2)                                        \n",
    "            self.batch2 = tf.layers.batch_normalization(self.pooling2)\n",
    "            self.relu2 = tf.nn.relu(self.batch2)\n",
    "            self.dropout2 = tf.nn.dropout(self.relu2, self.keep_prob)\n",
    "\n",
    "        # Convolutional Layer #1 and Pooling Layer #3\n",
    "        with tf.variable_scope('layer3'):\n",
    "            self.conv3 = tf.layers.conv2d(self.dropout2, 256, [1,3], padding='SAME')\n",
    "            self.pooling3 = tf.layers.max_pooling2d(inputs=self.conv3, pool_size=[1, 4] ,padding=\"VALID\", strides=2)   \n",
    "            self.batch3 = tf.layers.batch_normalization(self.pooling3)\n",
    "            self.relu3 = tf.nn.relu(self.batch3)\n",
    "            self.dropout3 = tf.nn.dropout(self.relu1, self.keep_prob)\n",
    "\n",
    "        # Dense Layer with Relu\n",
    "        with tf.variable_scope('layer4'):\n",
    "            self.flatten = tf.reshape(self.relu3, [-1, 12 * 256]) \n",
    "            self.dense1 = tf.layers.dense(inputs=self.flatten,\n",
    "                                     units=128, activation=tf.nn.relu)   \n",
    "            #Global Average Pooling\n",
    "            #self.GAP = tf.reduce_mean(self.relu3, axis=[1,2])# b,h,w,c\n",
    "            self.logits = tf.layers.dense(self.dense1,self.nb_classes)\n",
    "            self.prediction = tf.nn.softmax(self.logits)\n",
    "            \n",
    "        return self.logits\n",
    "\n",
    "\n",
    "    def train(self, data, target, keep_prob,is_dropout):\n",
    "        opt, cost ,acc = self.sess.run((self.optimizer, self.cost, self.accuracy ), \n",
    "                             feed_dict={self.y: target,\n",
    "                                        self.x: data,\n",
    "                                       self.keep_prob: keep_prob,\n",
    "                                       self.is_dropout : is_dropout})\n",
    "        return cost,acc\n",
    "\n",
    "    def test(self, data, target, keep_prob,is_dropout):\n",
    "        cost,acc = self.sess.run((self.cost,self.accuracy),\n",
    "                             feed_dict={self.y: target,\n",
    "                                        self.x: data,\n",
    "                                       self.keep_prob: keep_prob,\n",
    "                                       self.is_dropout : is_dropout})\n",
    "        return cost,acc\n",
    "\n",
    "    def get_conv_output(self, conv_output,data, keep_prob,is_dropout):\n",
    "        conv_output =  self.sess.run((conv_output), \n",
    "                             feed_dict={self.x: data,\n",
    "                                       self.keep_prob: keep_prob,\n",
    "                                       self.is_dropout : is_dropout})\n",
    "        return conv_output\n",
    "    \n",
    "    def predict(self, data, keep_prob,is_dropout):\n",
    "\n",
    "        prediction =  self.sess.run((self.prediction), \n",
    "                             feed_dict={self.x: data,\n",
    "                                       self.keep_prob: keep_prob,\n",
    "                                       self.is_dropout : is_dropout})\n",
    "        return prediction\n",
    "\n",
    "    def save(self, save_path='./model.ckpt'):\n",
    "        saved_path = self.saver.save(self.sess, save_path)\n",
    "        print(\"Model saved in file: %s\"%saved_path)\n",
    "\n",
    "    def load(self, load_path = './model.ckpt'):\n",
    "        self.saver.restore(self.sess, load_path)\n",
    "        print(\"Model restored\")\n",
    "\n",
    "    def terminate(self):\n",
    "        self.sess.close()\n",
    "        tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import unique\n",
    "nb_classes =trainY.shape[1]\n",
    "print (nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> turn off 구조는 마지막 convolution layer activation output 에 weight를 elementwise multiply 구조.\n",
    "train, test시에는 이 weight를 모두 1로 고정하며,\n",
    "prdiction 시에는 convolution layer activation output에서 상위 T%를 turn off 시키며 accuracy의 변화가 A 이상인 경우를\n",
    "threshold로 정하기 위해 이러한 모델 구조가 설정되었습니다.</font>\n",
    "\n",
    "<font color='blue'> CNN_MC_dropout_last_conv_turnoff</font> 모델은 상위 T%의 output을 가지는 last convolution output을 0으로 turn off 시키는 구조입니다.\n",
    "\n",
    "반면, <font color='blue'> CNN_MC_dropout_input_turnoff</font> 모델은 상위 T%의 output을 가지는 last convolution output에 매칭되는(하이라이트 되는 인풋)을 0으로 turn off 시키는 구조입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jny6_y8cxg6h"
   },
   "outputs": [],
   "source": [
    "if 'MC_dropout_model' in globals():\n",
    "    MC_dropout_model.terminate()\n",
    "    \n",
    "#MC_dropout_model =CNN_MC_dropout_input_turnoff(height=height,width= width, input_channel=input_channel ,nb_classes=nb_classes,\n",
    "#                                 std=std,batch_size=batch_size,l_rate=l_rate)\n",
    "\n",
    "MC_dropout_model =basic_CNN(height=height,width= width, input_channel=input_channel ,nb_classes=nb_classes,\n",
    "                                 std=std,batch_size=batch_size,l_rate=l_rate)\n",
    "\n",
    "#MC_dropout_model =CNN_MC_dropout(height=height,width= width, input_channel=input_channel ,nb_classes=nb_classes,\n",
    "#                                 std=std,batch_size=batch_size,l_rate=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i5mZbLexg6j",
    "outputId": "eb6c9052-804c-4e42-8eb5-e09b35fe7a10",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (MC_dropout_model.input)\n",
    "\n",
    "print ('[layer1] - MC dropout')\n",
    "print (MC_dropout_model.conv1)\n",
    "print (MC_dropout_model.pooling1)\n",
    "print (MC_dropout_model.batch1)\n",
    "print (MC_dropout_model.relu1)\n",
    "print (MC_dropout_model.dropout1)\n",
    "print ('[layer2] - MC dropout')\n",
    "print (MC_dropout_model.conv2)\n",
    "print (MC_dropout_model.pooling2)\n",
    "print (MC_dropout_model.batch2)\n",
    "print (MC_dropout_model.relu2)\n",
    "print (MC_dropout_model.dropout2)\n",
    "print ('[layer3]')\n",
    "print (MC_dropout_model.conv3)\n",
    "print (MC_dropout_model.pooling3)\n",
    "print (MC_dropout_model.batch3)\n",
    "print (MC_dropout_model.relu3)\n",
    "\n",
    "print ('[layer4]')\n",
    "print (MC_dropout_model.flatten)\n",
    "print (MC_dropout_model.dense1)\n",
    "print (MC_dropout_model.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LmWRfeSOxg6l"
   },
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pgl8PLBxg6m"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLBIHIbqxg6o",
    "outputId": "47abf10c-e76a-4462-a56c-3d0dbfd77cbd"
   },
   "outputs": [],
   "source": [
    "save_path = './model/'+data_name\n",
    "model_name = data_name+\"_CNN_MCdropout_basic_standardized_dropout_\"+str(l_rate)+'_'+str(batch_size)\n",
    "print (model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ism3OSKnxg6u"
   },
   "outputs": [],
   "source": [
    "from training import get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPy7CW6Ixg6w"
   },
   "outputs": [],
   "source": [
    "keep_prob =0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aSW6vp3xg6y",
    "outputId": "039e0277-40ba-439c-a344-dfbefd564aa5"
   },
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):#range(num_epochs):##\n",
    "    \n",
    "#     loss = 0\n",
    "#     acc=0\n",
    "#     train_batches = 0\n",
    "#     start_time = time.time()   \n",
    "    \n",
    "#     for batch in get_batch(X=trainX, Y=trainY, nb_classes = nb_classes, batch_size=batch_size):\n",
    "#         train_in, train_target = batch\n",
    "#         train_batches += 1.0 \n",
    "#         loss1 ,acc1 = MC_dropout_model.train(data=train_in,target= train_target,keep_prob=keep_prob,is_dropout=False)\n",
    "#         loss = loss1+loss\n",
    "#         acc = acc1+acc\n",
    "#         if np.isnan(loss):\n",
    "#             print ('error')\n",
    "#             break\n",
    "#     t_loss.append(loss/train_batches)\n",
    "#     t_acc.append(acc/train_batches)\n",
    "    \n",
    "#     train_history.loc[epoch] = [epoch+1, t_loss[epoch], t_acc[epoch] ,time.strftime(\"%Y-%m-%d-%H:%M\", time.localtime())]\n",
    "#     if not os.path.exists(os.path.join(save_path, model_name)):\n",
    "#         os.mkdir( os.path.join(save_path, model_name))\n",
    "#     MC_dropout_model.save(os.path.join(save_path, model_name,str(epoch+1)+'.ckpt'))\n",
    "\n",
    "    \n",
    "#     if(epoch+1)%val_freq ==0:\n",
    "#         acc=0\n",
    "#         loss = 0\n",
    "#         val_batches=0\n",
    "#         for batch in get_batch(X=validX, Y=validY, nb_classes = nb_classes, batch_size=batch_size):\n",
    "#             val_in, val_target = batch\n",
    "#             val_batches += 1.0 \n",
    "#             loss1 ,acc1 = MC_dropout_model.test(data=val_in,target= val_target,keep_prob=keep_prob,is_dropout=False)\n",
    "#             loss = loss1+loss\n",
    "#             acc = acc1+acc\n",
    "        \n",
    "#         v_acc.append(acc/val_batches)\n",
    "#         v_loss.append(loss/val_batches)\n",
    "#         valid_history.loc[epoch] = [epoch+1, v_loss[epoch], v_acc[epoch] ,time.strftime(\"%Y-%m-%d-%H:%M\", time.localtime())]  \n",
    "        \n",
    "#         print(\"  training loss:\\t{:.6f}\".format(t_loss[epoch]))\n",
    "#         print(\"  training acc:\\t{:.3f}\".format(t_acc[epoch]))\n",
    "#         print(\"  validation loss:\\t{:.6f}\".format(v_loss[epoch]))\n",
    "#         print(\"  validation acc:\\t{:.3f}\".format(v_acc[epoch]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rP08N40exg60"
   },
   "outputs": [],
   "source": [
    "# train_history.to_csv(os.path.join(save_path,model_name,  \"history_train.csv\"))\n",
    "# valid_history.to_csv(os.path.join(save_path,model_name,  \"history_valid.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlFgsib-xg64"
   },
   "source": [
    "# 5.check the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgcHlnjVxg65"
   },
   "source": [
    "### with loss and accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8-lRSQsxg66"
   },
   "outputs": [],
   "source": [
    "train_history = pd.read_csv(os.path.join(save_path,model_name,\"history_train.csv\"),index_col=0)\n",
    "valid_history = pd.read_csv(os.path.join(save_path,model_name,\"history_valid.csv\"),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(train_df = train_history,\n",
    "                   valid_df = valid_history,\n",
    "                   save_path = os.path.join(save_path,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9JDsIyvxg7B",
    "outputId": "126b0146-dbdc-4ca8-de1d-9b14572a8bf1"
   },
   "outputs": [],
   "source": [
    "bestepoch = int(np.argmin(valid_history['loss'].values))\n",
    "print( bestepoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Nve8UZsxg7D",
    "outputId": "5a50e4f4-3c80-441c-fe62-3f674ca92316"
   },
   "outputs": [],
   "source": [
    "error = valid_history.loc[bestepoch,'loss']\n",
    "print (\"error: {}\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogt5Ay-kxg7F",
    "outputId": "603808f4-930e-43cb-ecd5-597125a79523"
   },
   "outputs": [],
   "source": [
    "bestepoch=bestepoch\n",
    "MC_dropout_model.load(os.path.join(save_path,model_name,str(bestepoch)+'.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for epoch in [x+1 for x in range(num_epochs) if (x+1) !=bestepoch]:\n",
    "    for filename in glob.glob(os.path.join(save_path,model_name,str(epoch)+\".*\")):\n",
    "        os.remove(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5aT9WMWZxg7H"
   },
   "source": [
    "### with confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUz6EoWyxg7H"
   },
   "outputs": [],
   "source": [
    "from visualization import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnYhx8ffyEfa"
   },
   "outputs": [],
   "source": [
    "class_list = np.unique(trainy, return_counts=False, return_index=True)[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USnpndL2xg7K",
    "outputId": "c81932c3-151f-4d31-9b22-2d5e56514e74"
   },
   "outputs": [],
   "source": [
    "prediction = MC_dropout_model.predict(data=trainX,keep_prob=1.0,is_dropout=False)\n",
    "prediction = np.argmax(prediction,axis=1)\n",
    "target = np.argmax(trainY,axis=1)\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(target,prediction),                      \n",
    "    classes=class_list,\n",
    "    title=data_name+'classifcation(train)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HbKsurAxg7M",
    "outputId": "58e5d399-c33a-4a5f-e102-ad1b74e461ec"
   },
   "outputs": [],
   "source": [
    "prediction = MC_dropout_model.predict(data=testX,keep_prob=1.0,is_dropout=False)\n",
    "prediction = np.argmax(prediction,axis=1)\n",
    "target = np.argmax(testY,axis=1)\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(target,prediction),                      \n",
    "    classes=class_list,\n",
    "    title=data_name+'classifcation(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TONUyKSrxg7P"
   },
   "source": [
    "# 6. Trained 된 모델에서 Last Conv output Load하기(MC dropout적용안됨)\n",
    "기존논문에서 이미지타겟으로 p(A>T)=0.005 인것과 비교하여, 시계열에서는 조금 더 낮은 threshold가 패턴을 보기에 필요함.\n",
    "acc를 ~이상 떨어지게 하는 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_acc = MC_dropout_model.sess.run((MC_dropout_model.accuracy), \n",
    "                          feed_dict={MC_dropout_model.x: trainX,\n",
    "                                     MC_dropout_model.y: trainY,\n",
    "                                    MC_dropout_model.keep_prob:1.0,\n",
    "                                    MC_dropout_model.is_dropout:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output1 = MC_dropout_model.get_conv_output(conv_output = MC_dropout_model.relu1,data=trainX,keep_prob=1.0,is_dropout=False)[:,0,:,:]\n",
    "conv_output2 = MC_dropout_model.get_conv_output(conv_output = MC_dropout_model.relu2,data=trainX,keep_prob=1.0,is_dropout=False)[:,0,:,:]\n",
    "conv_output3 = MC_dropout_model.get_conv_output(conv_output = MC_dropout_model.relu3,data=trainX,keep_prob=1.0,is_dropout=False)[:,0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unit 에 적용할 threshold를 정하고, 각 conv에서 unit끼리의 IOU 수치 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> threshold를 정할 때, 해당 T% 이상인 last convolution activation output을 turnoff 시킬때의 accuracy 변화가 A이상일 때를 기준으로 합니다 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold는 우선 임시로 상위 5%로 정해놓았습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = np.percentile((conv_output1.flatten()),100 - A)\n",
    "threshold2 = np.percentile((conv_output2.flatten()),100 - A)\n",
    "threshold3 = np.percentile((conv_output3.flatten()),100 - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_bool1 = conv_output1 > threshold1\n",
    "conv_bool2 = conv_output2 > threshold2\n",
    "conv_bool3 = conv_output3 > threshold3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,3,1)\n",
    "ax.imshow(conv_bool1[3].T,aspect='auto')\n",
    "ax = plt.subplot(1,3,2)\n",
    "ax.imshow(conv_bool2[0].T,aspect='auto')\n",
    "ax = plt.subplot(1,3,3)\n",
    "ax.imshow(conv_bool3[0].T,aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Dissetion Code\n",
    "\n",
    "for x, index in enumerate(top[unit]):\n",
    "    row = x // gridwidth\n",
    "    col = x % gridwidth\n",
    "    image = imread(ds.filename(index))\n",
    "    mask = imresize(features[index][unit], image.shape[:2], mode='F')\n",
    "    mask = mask > thresholds[unit]\n",
    "    vis = (mask[:, :, numpy.newaxis] * 0.8 + 0.2) * image\n",
    "    if vis.shape[:2] != (imsize, imsize):\n",
    "        vis = imresize(vis, (imsize, imsize))\n",
    "    tiled[row*(imsize+gap):row*(imsize+gap)+imsize,\n",
    "          col*(imsize+gap):col*(imsize+gap)+imsize,:] = vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling(y, target_len):\n",
    "    newx = np.arange(0, target_len)\n",
    "    tempx = np.linspace(0, target_len, len(y))\n",
    "    newy = np.interp(newx, tempx, y)\n",
    "    return newy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output3_extend= np.zeros((500, 117, 256))\n",
    "for d_idx in range(500):\n",
    "    for c_idx  in range(256):\n",
    "        conv_output3_extend[d_idx, :, c_idx] = upsampling(conv_output3[d_idx, :, c_idx], 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_bool3_extend = conv_output3_extend > threshold3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,3])\n",
    "plt.imshow(conv_bool3[0,:,:20].T > threshold3)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,5])\n",
    "plt.imshow(conv_output3_extend[0,:,:20].T > threshold3)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import more_itertools as mit\n",
    "\n",
    "plot_idx = 0\n",
    "d_idx = 0\n",
    "input_ch =  0\n",
    "for conv_channel in range(256):\n",
    "    plot_idx+=1\n",
    "    fig, ax= plt.subplots(figsize=(15,3))\n",
    "    ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "    spotlight = conv_bool3_extend[d_idx,:,conv_channel]\n",
    "\n",
    "    # 데이터 자체 그리기\n",
    "    data = trainX[d_idx,0,:,input_ch]\n",
    "    ax.plot(data,color='black')\n",
    "    ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "\n",
    "    for spotlight_se in [(list(group)) for group in mit.consecutive_groups([i for i,x in enumerate(conv_bool3_extend[d_idx,:,conv_channel]) if x==True])]:\n",
    "        ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "\n",
    "    if plot_idx ==50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마스크 자체를 업샘플링\n",
    "plt.imshow([upsampling(conv_bool3[0,:,x],117) for x in range(256)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_bool3_extend= np.zeros((500, 117, 256))\n",
    "for d_idx in range(500):\n",
    "    for c_idx  in range(256):\n",
    "        conv_bool3_extend[d_idx, :, c_idx] = (upsampling(conv_bool3[d_idx, :, c_idx], 117) > 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv_bool3_extend[0].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_bool3_extend[0,:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패턴 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pattern_channel =0 \n",
    "pattern_length = 10\n",
    "consecutive_pattern=[]\n",
    "step_size = 10\n",
    "for data_idx,data in enumerate(trainX[:500,:,:,[input_pattern_channel]]):\n",
    "    for output_channel_idx in range(256):\n",
    "        highlight_idx = [p for p, x in enumerate(conv_bool3_extend[data_idx][:,output_channel_idx]) if x]\n",
    "        if highlight_idx:\n",
    "            groups = []\n",
    "            for _, g in groupby(highlight_idx, key=lambda n, c=itertools.count(): n-next(c)):\n",
    "                groups.append(list(g))    # Store group iterator as a list\n",
    "            for g in groups:\n",
    "                if len(g)>=pattern_length:\n",
    "                    for start_idx in range(0,len(g)-pattern_length+1,step_size):\n",
    "                        short_pattern_idx = [g[x] for x in range(start_idx, start_idx+pattern_length)]\n",
    "                        # 같은 데이터의 같은 부분이 패턴으로 인식될 때, repetitive_flag가 TRUE로 변환\n",
    "                        repetitive_flag=False\n",
    "                        for x in consecutive_pattern:\n",
    "                            if (x['data_idx']==data_idx and x['pattern_idx'] == short_pattern_idx):\n",
    "                                x['pattern_channel'].append(output_channel_idx)\n",
    "                                repetitive_flag=True\n",
    "                        if not repetitive_flag:\n",
    "                            consecutive_pattern.append({\"data_idx\":data_idx, \"pattern_idx\":short_pattern_idx, \"pattern\":data[0,short_pattern_idx].flatten(), \"pattern_channel\":[output_channel_idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['pattern'] for x in consecutive_pattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "normalized_pattern_candidate =TimeSeriesScalerMeanVariance().fit_transform([x['pattern'] for x in consecutive_pattern])\n",
    "normalized_pattern_candidate = normalized_pattern_candidate.squeeze()\n",
    "normalized_pattern_candidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import cluster, mixture\n",
    "\n",
    "# pseudoF_gmm = []\n",
    "\n",
    "# cut_num=20\n",
    "# for n_clusters in range(3,cut_num):\n",
    "#     gmm = mixture.GaussianMixture(n_components=n_clusters, \n",
    "#             covariance_type='full').fit(normalized_pattern_candidate)\n",
    "#     labels = gmm.predict(normalized_pattern_candidate)\n",
    "#     pseudoF_gmm.append(metrics.calinski_harabasz_score(normalized_pattern_candidate, labels))\n",
    "\n",
    "# x = np.arange(3,cut_num)\n",
    "# plt.plot(x, pseudoF_gmm[:cut_num])\n",
    "# plt.xticks(x)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudoF_kmeans = []\n",
    "\n",
    "# cut_num=20\n",
    "# for n_clusters in range(3,cut_num):\n",
    "#     kmeans_model = cluster.KMeans(n_clusters=n_clusters, random_state=1).fit(normalized_pattern_candidate)\n",
    "#     labels = kmeans_model.labels_\n",
    "#     pseudoF_kmeans.append(metrics.calinski_harabasz_score(normalized_pattern_candidate, labels))\n",
    "\n",
    "# x = np.arange(3,cut_num)\n",
    "# plt.plot(x, pseudoF_kmeans[:cut_num])\n",
    "# plt.xticks(x)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_g_clusters = 4\n",
    "best_k_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_cluster : interger\n",
    "#eigen_solver{None, ‘arpack’, ‘lobpcg’, or ‘amg’}\n",
    "#affinity: ‘nearest_neighbors’‘rbf’\n",
    "\n",
    "\n",
    "#covariance_type{‘full’ (default), ‘tied’, ‘diag’, ‘spherical’}\n",
    "default_gmm = {'n_clusters':2, 'covariance_type': 'full'}\n",
    "default_kmeans = {'n_clusters':2,'verbose':False,'random_state':0}\n",
    "default_hac = {'method':'single','metric':'euclidean','cut_off_level':5,'criterion':'maxclust'}\n",
    "\n",
    "gmm_option = [\n",
    "    ({'n_clusters': best_g_clusters})\n",
    "]\n",
    "\n",
    "kmeans_option = [\n",
    "    ({'n_clusters': best_k_clusters})\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_cluster_mean_pattern = []\n",
    "k_cluster_mean_pattern = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = [\"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\",\n",
    "        \"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\"]\n",
    "\n",
    "random.seed(300)\n",
    "for g_option,k_option in zip(gmm_option,kmeans_option):\n",
    "\n",
    "    g_params = default_gmm.copy()\n",
    "    g_params.update(g_option)\n",
    "    \n",
    "    k_params = default_kmeans.copy()\n",
    "    k_params.update(k_option)\n",
    "    \n",
    "\n",
    "    gmm =  mixture.GaussianMixture(\n",
    "        n_components=g_params['n_clusters'], \n",
    "        covariance_type=g_params['covariance_type'])\n",
    "    \n",
    "    kmeans = cluster.KMeans(\n",
    "        n_clusters=k_params['n_clusters'], \n",
    "        random_state=k_params['random_state'])\n",
    "\n",
    "    g_pred = gmm.fit_predict(normalized_pattern_candidate)\n",
    "    k_pred = kmeans.fit_predict(normalized_pattern_candidate)\n",
    "    \n",
    "    g_cluster_mean_pattern =[]\n",
    "    k_cluster_mean_pattern =[]\n",
    "\n",
    "    fig = plt.figure(figsize=(len(np.unique(g_pred))*3,7))\n",
    "    axes = fig.subplots(2, g_params['n_clusters'])\n",
    "    for class_ in range(0,g_params['n_clusters']):\n",
    "        g_class_idx = [i for i,x in enumerate(g_pred) if x ==class_]\n",
    "        k_class_idx = [i for i,x in enumerate(k_pred) if x ==class_]\n",
    "\n",
    "        try:\n",
    "            axes[0,class_].plot(normalized_pattern_candidate[random.sample(g_class_idx,30)].T, color =color[class_],alpha=0.6)\n",
    "            axes[1,class_].plot(normalized_pattern_candidate[random.sample(k_class_idx,30)].T, color =color[class_],alpha=0.6)\n",
    "\n",
    "        except:\n",
    "            axes[0,class_].plot(normalized_pattern_candidate[g_class_idx].T, color =color[class_],alpha=0.6)\n",
    "            axes[1,class_].plot(normalized_pattern_candidate[k_class_idx].T, color =color[class_],alpha=0.6)\n",
    "\n",
    "        g_cluster_mean = normalized_pattern_candidate[g_class_idx].mean(axis=0)\n",
    "        g_cluster_mean_pattern.append(g_cluster_mean)\n",
    "        k_cluster_mean = normalized_pattern_candidate[k_class_idx].mean(axis=0)\n",
    "        k_cluster_mean_pattern.append(k_cluster_mean)\n",
    "        axes[0,class_].plot(g_cluster_mean, color ='black',alpha=1,linewidth=5)\n",
    "        axes[1,class_].plot(k_cluster_mean, color ='black',alpha=1,linewidth=5)\n",
    "\n",
    "    #fig.suptitle(\"Spectral Clustering: {} \\nGMM: {}\".format(s_params,g_params),y=0.98)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import more_itertools as mit\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = MC_dropout_model.predict(data=trainX,keep_prob=1.0,is_dropout=False)\n",
    "sample_data_idx = [i for i,x in enumerate(prediction) if np.argmax(x)==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(trainX[0,0,:,0])\n",
    "\n",
    "spotlight = conv_bool3_extend[d_idx,:,p_channel]\n",
    "for spotlight_se in [(list(group)) for group in mit.consecutive_groups([i for i,x in enumerate(spotlight) if x==True])]:\n",
    "    ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx = 0\n",
    "d_idx = 0\n",
    "input_ch =  0\n",
    "specific_pattern_idx=[i for i,x in enumerate(consecutive_pattern) if x['data_idx'] ==d_idx]\n",
    "for idx, p_info in enumerate(specific_pattern_info):\n",
    "    for out_channel in range(256):\n",
    "        plot_idx+=1\n",
    "\n",
    "        fig, ax= plt.subplots(figsize=(15,3))\n",
    "        ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "        spotlight = conv_bool3_extend[d_idx,:,out_channel]\n",
    "\n",
    "        # 데이터 자체 그리기\n",
    "        data = trainX[d_idx,0,:,input_channel]\n",
    "        ax.plot(data,color='black')\n",
    "        ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "\n",
    "        for spotlight_se in [(list(group)) for group in mit.consecutive_groups([i for i,x in enumerate(spotlight) if x==True])]:\n",
    "            ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "\n",
    "        # 데이터의 해당 패턴 그리기\n",
    "\n",
    "        pattern_x = p_info['pattern_idx']\n",
    "        #color_idx = g_pred[specific_len_pattern_idx.index(pattern_idx)]\n",
    "        #pattern = consecutive_pattern[pattern_idx]['pattern']\n",
    "        ax.plot(pattern_x, data[pattern_x], color = 'r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flag = 0 \n",
    "d_idx = 0\n",
    "#for d_idx in sample_data_idx:\n",
    "######## 특정 샘플의 패턴 리스트 생성  \n",
    "# sample_data_idx의 전체 패턴\n",
    "specific_pattern_idx=[i for i,x in enumerate(consecutive_pattern) if x['data_idx'] ==d_idx]\n",
    "specific_pattern_info = [consecutive_pattern[x] for x in specific_pattern_idx]\n",
    "\n",
    "#print(len(specific_pattern_info))\n",
    "\n",
    "for idx, p_info in enumerate(specific_pattern_info):\n",
    "    flag += 1\n",
    "    pattern_idx = specific_pattern_idx[idx]\n",
    "    fig, ax= plt.subplots(figsize=(15,3))\n",
    "    for p_channel in range(256):\n",
    "\n",
    "\n",
    "        ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "\n",
    "        # 데이터 자체 그리기\n",
    "        data = trainX[d_idx,0,:,0]\n",
    "        ax.plot(data,color='black')\n",
    "        ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "\n",
    "        ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "        spotlight = conv_bool3_extend[d_idx,:,p_channel]\n",
    "        for spotlight_se in [(list(group)) for group in mit.consecutive_groups([x for i,x in enumerate(spotlight) if x.any()==True])]:\n",
    "            ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "\n",
    "#                 # 데이터의 해당 패턴 그리기\n",
    "#                 color_idx = g_pred[specific_pattern_idx.index(pattern_idx)]\n",
    "#                 pattern_x = consecutive_pattern[pattern_idx]['pattern_idx']\n",
    "#                 #pattern = consecutive_pattern[pattern_idx]['pattern']\n",
    "#                 ax.plot(pattern_x, data[pattern_x], color = color[color_idx])\n",
    "\n",
    "#                 # 패턴의 평균값 굵고 투명하게 그리기\n",
    "#                 x = g_cluster_mean_pattern[color_idx].reshape(-1,1) #returns a numpy array\n",
    "#                 min_max_scaler = MinMaxScaler(feature_range=(data[pattern_x].min(), data[pattern_x].max()))\n",
    "#                 x_scaled = min_max_scaler.fit_transform(x)\n",
    "#                 ax.plot(pattern_x, x_scaled, color = color[color_idx], alpha=0.1,linewidth=5)\n",
    "#             fig.suptitle(\"prediction{} data{}, output_channel{}\".format(np.argmax(prediction[d_idx]),d_idx,\\,p_info['pattern_channel']))\n",
    "    if flag == 10 :\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channel =0\n",
    "plot_idx = 0\n",
    "d_idx = 0\n",
    "\n",
    "# for d_idx in sample_data_idx:\n",
    "######## 특정 샘플의 패턴 리스트 생성  \n",
    "# sample_data_idx의 전체 패턴\n",
    "specific_data_pattern=[x for i,x in enumerate(consecutive_pattern) if x['data_idx'] ==0]\n",
    "\n",
    "#print(len(specific_pattern_info))\n",
    "\n",
    "for idx, p_info in enumerate(specific_data_pattern):\n",
    "    specific_data_pattern[idx]\n",
    "    fig, ax= plt.subplots(figsize=(15,3))\n",
    "    for p_channel in range(256):\n",
    "\n",
    "        # 데이터 자체 그리기\n",
    "        data = trainX[d_idx,0,:,input_channel]\n",
    "        ax.plot(data,color='black')\n",
    "        ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "\n",
    "        # 하이라이트 그리기\n",
    "        ax.set_xlim([0,int(trainX.shape[2])-1])         \n",
    "        spotlight = conv_bool3_extend[d_idx,:,p_channel]\n",
    "\n",
    "        pattern_x = consecutive_pattern[idx]['pattern']\n",
    "#         color_idx = g_pred[specific_len_pattern_idx.index(pattern_idx)]\n",
    "        ax.plot(pattern_x, data[pattern_x], color = color[color_idx])\n",
    "\n",
    "#             # 패턴의 평균값 굵고 투명하게 그리기\n",
    "#             x = g_cluster_mean_pattern[color_idx].reshape(-1,1) #returns a numpy array\n",
    "#             min_max_scaler = MinMaxScaler(feature_range=(data[pattern_x].min(), data[pattern_x].max()))\n",
    "#             x_scaled = min_max_scaler.fit_transform(x)\n",
    "#             ax.plot(pattern_x, x_scaled, color = color[color_idx], alpha=0.4,linewidth=10)\n",
    "#         fig.suptitle(\"prediction{} data{}, output_channel{}\".format(np.argmax(prediction[d_idx]),d_idx,p_info['pattern_channel']))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_data_pattern=[x for i,x in enumerate(consecutive_pattern) if x['data_idx'] ==0]\n",
    "\n",
    "#print(len(specific_pattern_info))\n",
    "\n",
    "for idx, p_info in enumerate(specific_data_pattern):\n",
    "    specific_data_pattern[idx]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx = 0\n",
    "d_idx = 0\n",
    "input_ch =  0\n",
    "\n",
    "\n",
    "for out_channel in range(256):\n",
    "    plot_idx+=1\n",
    "        \n",
    "    fig, ax= plt.subplots(figsize=(15,3))\n",
    "    ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "    spotlight = conv_bool3_extend[d_idx,:,out_channel]\n",
    "\n",
    "    # 데이터 자체 그리기\n",
    "    data = trainX[d_idx,0,:,input_channel]\n",
    "    ax.plot(data,color='black')\n",
    "    ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "\n",
    "    for spotlight_se in [(list(group)) for group in mit.consecutive_groups([i for i,x in enumerate(spotlight) if x==True])]:\n",
    "        ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "        \n",
    "    # 데이터의 해당 패턴 그리기\n",
    "    \n",
    "    pattern_x = consecutive_pattern[pattern_idx]['pattern_idx']\n",
    "    color_idx = g_pred[specific_len_pattern_idx.index(pattern_idx)]\n",
    "    #pattern = consecutive_pattern[pattern_idx]['pattern']\n",
    "    ax.plot(pattern_x, data[pattern_x], color = color[color_idx])\n",
    "\n",
    "    # 패턴의 평균값 굵고 투명하게 그리기\n",
    "    x = g_cluster_mean_pattern[color_idx].reshape(-1,1) #returns a numpy array\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(data[pattern_x].min(), data[pattern_x].max()))\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    ax.plot(pattern_x, x_scaled, color = color[color_idx], alpha=0.4,linewidth=10)\n",
    "\n",
    "    if plot_idx ==50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = MC_dropout_model.predict(data=trainX,keep_prob=1.0,is_dropout=False)\n",
    "sample_data_idx = [i for i,x in enumerate(prediction) if np.argmax(x)==0]\n",
    "\n",
    "for d_idx in sample_data_idx:\n",
    "######## 특정 샘플의 패턴 리스트 생성  \n",
    "    # sample_data_idx의 전체 패턴\n",
    "    specific_data_pattern_idx=[i for i,x in enumerate(consecutive_pattern) if x['data_idx'] ==d_idx]\n",
    "    specific_pattern_idx= list(set(specific_data_pattern_idx))\n",
    "    specific_pattern_info = [consecutive_pattern[x] for x in specific_pattern_idx]\n",
    "    \n",
    "    #print(len(specific_pattern_info))\n",
    "    \n",
    "    for idx, p_info in enumerate(specific_pattern_info):\n",
    "        pattern_idx = specific_pattern_idx[idx]\n",
    "        fig, ax= plt.subplots(figsize=(15,3))\n",
    "        #for p_channel in np.unique([x['pattern_channel'] for x in specific_pattern_info]):\n",
    "        for p_channel in range(256):\n",
    "            \n",
    "            ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "            \n",
    "            spotlight = conv_bool3_extend[d_idx,:,p_channel]\n",
    "            for spotlight_se in [(list(group)) for group in mit.consecutive_groups([x for i,x in enumerate(spotlight)])]:\n",
    "                ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "\n",
    "            # 데이터 자체 그리기\n",
    "            data = trainX[d_idx,0,:,input_channel]\n",
    "            ax.plot(data,color='black')\n",
    "            ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "        \n",
    "            # 데이터의 해당 패턴 그리기\n",
    "            color_idx = g_pred[specific_pattern_idx.index(pattern_idx)]\n",
    "            pattern_x = consecutive_pattern[pattern_idx]['pattern_idx']\n",
    "            #pattern = consecutive_pattern[pattern_idx]['pattern']\n",
    "            ax.plot(pattern_x, data[pattern_x], color = color[color_idx])\n",
    "\n",
    "            # 패턴의 평균값 굵고 투명하게 그리기\n",
    "            x = g_cluster_mean_pattern[color_idx].reshape(-1,1) #returns a numpy array\n",
    "            min_max_scaler = MinMaxScaler(feature_range=(data[pattern_x].min(), data[pattern_x].max()))\n",
    "            x_scaled = min_max_scaler.fit_transform(x)\n",
    "            ax.plot(pattern_x, x_scaled, color = color[color_idx], alpha=0.4,linewidth=10)\n",
    "        fig.suptitle(\"prediction{} data{}, output_channel{}\".format(np.argmax(prediction[d_idx]),d_idx,p_info['pattern_channel']))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3_pattern=[]\n",
    "for data_idx,data in enumerate(trainX[:500,:,:,:]):\n",
    "    for output_channel_idx in range(conv_bool3_extend.shape[2]):\n",
    "        highlight_idx = [p for p, x in enumerate(conv_bool3_extend[data_idx][:,output_channel_idx]) if x]\n",
    "        if highlight_idx:\n",
    "            groups=[]\n",
    "            for _, g in groupby(highlight_idx, key=lambda n, c=itertools.count(): n-next(c)):\n",
    "                groups.append(list(g))    # Store group iterator as a list\n",
    "            for g in groups:\n",
    "                for data_channel in range(trainX.shape[3]):\n",
    "                    conv3_pattern.append({\"data_idx\":data_idx,\"input_channel\": data_channel,\n",
    "                                          \"conv_channel\": output_channel_idx,  \"pattern_idx\":g,\"pattern\":data[0,g].flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs_pattern=[]\n",
    "stride_size=2\n",
    "pooling_size=4\n",
    "for data_idx,data in enumerate(trainX[:500,:,:,:]):\n",
    "    for conv_idx in range(3,4):\n",
    "        conv_bool = eval(\"conv_bool\"+str(conv_idx))\n",
    "        for conv_channel in range(conv_bool.shape[2]):\n",
    "            pattern_highlight_idx = [p for p,x in enumerate(conv_bool[data_idx,:,conv_channel]) if x]\n",
    "            input_highlight_idx=[np.arange(stride_size*p, (stride_size*p)+pow(pooling_size,conv_idx)) for p in pattern_highlight_idx]\n",
    "            input_highlight_idx_extend=np.sort(list(set(np.asarray(input_highlight_idx).flatten())))\n",
    "            groups=[]\n",
    "            for _, g in groupby(input_highlight_idx_extend, key=lambda n, c=itertools.count(): n-next(c)):\n",
    "                groups.append(list(g))    # Store group iterator as a list\n",
    "            for g in groups:\n",
    "                for data_channel in range(input_channel):\n",
    "                    convs_pattern.append({\"data_idx\":data_idx,\"input_channel\": data_channel,\n",
    "                                          \"conv_idx\":conv_idx,\"conv_channel\": conv_channel,\n",
    "                                          \"input_highlight_idx\":input_highlight_idx,\"input_highlight_idx_extend\":input_highlight_idx_extend,\n",
    "                                          \"upscaled_pattern_idx\":g,  \"pattern\":data[0,g,data_channel].flatten()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv 끼리 모으려면 아래의 코드를 돌리시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_pattern = [x for x in convs_pattern if x['conv_channel']==1]\n",
    "conv2_pattern = [x for x in convs_pattern if x['conv_channel']==2]\n",
    "conv3_pattern = [x for x in convs_pattern if x['conv_channel']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in convs_pattern if (x['data_idx']==0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx = 0\n",
    "d_idx = 0\n",
    "input_ch =  0\n",
    "for conv_channel in range(256):\n",
    "    plot_idx+=1\n",
    "        \n",
    "    fig, ax= plt.subplots(figsize=(15,3))\n",
    "    ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "    spotlight = conv_bool3[d_idx,:,conv_channel]\n",
    "\n",
    "    # 데이터 자체 그리기\n",
    "    data = trainX[d_idx,0,:,input_channel]\n",
    "    ax.plot(data,color='black')\n",
    "    ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "\n",
    "    for spotlight_se in [(list(group)) for group in mit.consecutive_groups([i for i,x in enumerate(spotlight) if x==True])]:\n",
    "        ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "\n",
    "    if plot_idx ==50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import more_itertools as mit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx = 0\n",
    "\n",
    "for d_idx in sample_data_idx:\n",
    "######## 특정 샘플의 패턴 리스트 생성  \n",
    "#     # sample_data_idx의 전체 패턴\n",
    "#     specific_data_pattern_idx=[i for i,x in enumerate(conv3_pattern) if x['data_idx'] ==d_idx]\n",
    "#     specific_pattern_info = [consecutive_pattern[x] for x in specific_data_pattern_idx]\n",
    "    \n",
    "#     #해당 데이터에 미리 정한 길이만큼의 패턴이 없을 경우 pass\n",
    "#     if len(specific_pattern_info):\n",
    "#         pass\n",
    "        \n",
    "    # 총 10개의 plot을 그릴때까지 진행\n",
    "    if plot_idx ==5:\n",
    "        break\n",
    "    \n",
    "    for p_channel in np.unique([(x['pattern_channel']) for x in specific_pattern_info if (x['data_idx']==d_idx)]):\n",
    "        plot_idx+=1\n",
    "        fig, ax= plt.subplots(figsize=(15,3))\n",
    "        ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "        spotlight = conv_bool[d_idx,:,conv_channel]\n",
    "        \n",
    "        # 데이터 자체 그리기\n",
    "        data = trainX[d_idx,0,:,input_channel]\n",
    "        ax.plot(data,color='black')\n",
    "        ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "            \n",
    "        for spotlight_se in [(list(group)) for group in mit.consecutive_groups([i for i,x in enumerate(spotlight) if x==True])]:\n",
    "            ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "                \n",
    "#         for idx,pattern_x, pattern in [(i,x['pattern_idx'],x['pattern']) for i,x in enumerate(specific_pattern_info) if (x['data_idx']==d_idx  and x['pattern_channel']== p_channel)]:\n",
    "#             # 데이터의 해당 패턴 그리기\n",
    "#             pattern_idx = specific_data_pattern_idx[idx]\n",
    "#             color_idx = gak_km_predict[specific_data_pattern_idx.index(pattern_idx)]\n",
    "#             pattern_x = consecutive_pattern[pattern_idx]['pattern_idx']\n",
    "#             #pattern = consecutive_pattern[pattern_idx]['pattern']\n",
    "#             ax.plot(pattern_x, data[pattern_x], color = color[color_idx])\n",
    "#         fig.suptitle(\"prediction{}, data{}, output_channel{}\".format(np.argmax(prediction[d_idx]),d_idx,consecutive_pattern[pattern_idx]['pattern_channel']))\n",
    "#         break\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.unique(np.asarray(pattern_len), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_len = 64\n",
    "specific_len_pattern_idx=[i for i,x in enumerate(conv3_pattern) if len(x['pattern']) ==pattern_len]\n",
    "specific_pattern=[x['pattern'] for i,x in enumerate(conv3_pattern) if i in specific_len_pattern_idx]\n",
    "len(specific_len_pattern_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_pattern[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the best clusterer num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import cluster, mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudoF_kmeans = []\n",
    "\n",
    "for n_clusters in range(2,len(specific_pattern)):\n",
    "    kmeans_model = cluster.KMeans(n_clusters=n_clusters, random_state=1).fit(specific_pattern)\n",
    "    labels = kmeans_model.labels_\n",
    "    pseudoF_kmeans.append(metrics.calinski_harabasz_score(specific_pattern, labels))\n",
    "\n",
    "plt.plot(pseudoF_kmeans)\n",
    "plt.show()\n",
    "\n",
    "cut_num=20\n",
    "x = np.arange(2,128)\n",
    "plt.plot(x[:cut_num], pseudoF_kmeans[:cut_num])\n",
    "plt.xticks(x[:cut_num])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Focus on 2~20\n",
    "\n",
    "cut_num=20\n",
    "x = np.arange(2,128)\n",
    "plt.plot(x[:cut_num], pseudoF_kmeans[:cut_num])\n",
    "plt.xticks(x[:cut_num])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import cluster, mixture\n",
    "\n",
    "pseudoF_gmm = []\n",
    "\n",
    "for n_clusters in range(2,len(specific_len_pattern_idx)):\n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters, \n",
    "            covariance_type='full').fit(specific_pattern)\n",
    "    labels = gmm.predict(specific_pattern)\n",
    "    pseudoF_gmm.append(metrics.calinski_harabasz_score(specific_pattern, labels))\n",
    "\n",
    "plt.plot(pseudoF_gmm)\n",
    "plt.show()\n",
    "\n",
    "cut_num=20\n",
    "x = np.arange(2,128)\n",
    "plt.plot(x[:cut_num], pseudoF_gmm[:cut_num])\n",
    "plt.xticks(x[:cut_num])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_num=20\n",
    "x = np.arange(2,128)\n",
    "plt.plot(x[:cut_num], pseudoF_gmm[:cut_num])\n",
    "plt.xticks(x[:cut_num])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_clusters=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with pseudoF num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_cluster : interger\n",
    "#eigen_solver{None, ‘arpack’, ‘lobpcg’, or ‘amg’}\n",
    "#affinity: ‘nearest_neighbors’‘rbf’\n",
    "\n",
    "\n",
    "#covariance_type{‘full’ (default), ‘tied’, ‘diag’, ‘spherical’}\n",
    "default_gmm = {'n_clusters':2, 'covariance_type': 'full'}\n",
    "default_kmeans = {'n_clusters':2,'verbose':False,'random_state':0}\n",
    "default_hac = {'method':'single','metric':'euclidean','cut_off_level':5,'criterion':'maxclust'}\n",
    "\n",
    "gmm_option = [\n",
    "    ({'n_clusters': best_n_clusters})\n",
    "]\n",
    "\n",
    "kmeans_option = [\n",
    "    ({'n_clusters': best_n_clusters})\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(specific_pattern[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_cluster_mean_pattern = []\n",
    "k_cluster_mean_pattern = []\n",
    "\n",
    "g_cluster_pattern_list = []\n",
    "k_cluster_pattern_list = []\n",
    "\n",
    "gmm_option = [({'n_clusters': best_n_clusters })]\n",
    "kmeans_option = [({'n_clusters': best_n_clusters})]\n",
    "\n",
    "for g_option,k_option in zip(gmm_option,kmeans_option):\n",
    "\n",
    "    g_params = default_gmm.copy()\n",
    "    g_params.update(g_option)\n",
    "\n",
    "    k_params = default_kmeans.copy()\n",
    "    k_params.update(k_option)\n",
    "\n",
    "    gmm =  mixture.GaussianMixture(\n",
    "        n_components=g_params['n_clusters'], \n",
    "        covariance_type=g_params['covariance_type'])\n",
    "\n",
    "    kmeans = cluster.KMeans(\n",
    "        n_clusters=k_params['n_clusters'], \n",
    "        random_state=k_params['random_state'])\n",
    "\n",
    "    g_pred = gmm.fit_predict(specific_pattern)\n",
    "    k_pred = kmeans.fit_predict(specific_pattern)\n",
    "\n",
    "    fig = plt.figure(figsize=(len(np.unique(g_pred))*3,7))\n",
    "    axes = fig.subplots(2, g_params['n_clusters'])\n",
    "\n",
    "    for class_ in range(0,g_params['n_clusters']):\n",
    "        g_class_idx = [i for i,x in enumerate(g_pred) if x ==class_]\n",
    "        k_class_idx = [i for i,x in enumerate(k_pred) if x ==class_]\n",
    "        \n",
    "        g_class_pattern = np.array([specific_pattern[idx] for idx in g_class_idx])\n",
    "        k_class_pattern = np.array([specific_pattern[idx] for idx in k_class_idx])\n",
    "\n",
    "        g_cluster_pattern_list.append(g_class_pattern)\n",
    "        k_cluster_pattern_list.append(k_class_pattern)\n",
    "        \n",
    "        axes[0,class_].plot(g_class_pattern.T, color =color[class_],alpha=0.6)\n",
    "        axes[1,class_].plot(k_class_pattern.T, color =color[class_],alpha=0.6)\n",
    "\n",
    "        g_cluster_mean = g_class_pattern.mean(axis=0)\n",
    "        g_cluster_mean_pattern.append({\"class_\":class_,\"cluster_mean\":g_cluster_mean})\n",
    "        k_cluster_mean = k_class_pattern.mean(axis=0)\n",
    "        k_cluster_mean_pattern.append({\"class_\":class_,\"cluster_mean\":k_cluster_mean})\n",
    "\n",
    "        axes[0,class_].plot(g_cluster_mean, color ='black',alpha=1,linewidth=5)\n",
    "        axes[1,class_].plot(k_cluster_mean, color ='black',alpha=1,linewidth=5)\n",
    "#                 axes[2,class_].plot(s_cluster_mean, color ='black',alpha=1,linewidth=5)\n",
    "\n",
    "    #(\"KMeans: {} \\nGMM: {}\".format(k_params,g_params),y=1.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removeing Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_25_pattern = np.asarray([np.percentile(data, idx, axis=1, interpolation = 'midpoint') for idx in range(0, 25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(top_25_pattern.T)\n",
    "plt.plot(top_25_pattern.mean(axis=0), linewidth=5, color = 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_75_pattern = np.asarray([np.percentile(data, idx, axis=1, interpolation = 'midpoint') for idx in range(75, 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(top_75_pattern.T)\n",
    "plt.plot(top_75_pattern.mean(axis=0), linewidth=5, color = 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtwalign import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data = g_cluster_mean_pattern[0]['cluster_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stand_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_pattens=[]\n",
    "for i, d in enumerate(data):\n",
    "    res = dtw(stand_data,d)\n",
    "    dis_pattens.append({\"idx\":i, \"distance\":res.normalized_distance, \"data\":d})\n",
    "    \n",
    "distandce_list = np.argsort([dis_pattens[idx]['distance'] for idx in range(len(dis_pattens))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in distandce_list:\n",
    "    plt.plot(dis_pattens[idx]['data'])\n",
    "    plt.title(\"DTW score:\"+str(dis_pattens[idx]['distance']))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_pattens=[]\n",
    "for i, d in enumerate(data):\n",
    "    res = dtw(d, stand_data)\n",
    "    dis_pattens.append({\"idx\":i, \"distance\":res.normalized_distance, \"data\":d})\n",
    "    \n",
    "distandce_list = np.argsort([dis_pattens[idx]['distance'] for idx in range(len(dis_pattens))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dis_pattens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in distandce_list:\n",
    "    plt.plot(dis_pattens[idx]['data'])\n",
    "    plt.title(\"DTW score:\"+str(dis_pattens[idx]['distance']))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distandce_list_threshold = np.argsort([dis_pattens[idx]['distance'] for idx in range(len(dis_pattens)) if dis_pattens[idx]['distance'] < 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list = np.array([dis_pattens[idx]['data'] for idx in range(len(dis_pattens)) if dis_pattens[idx]['distance'] < 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(threshold_list.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(threshold_list.mean(axis=0), color='r')\n",
    "plt.plot(stand_data, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx = 0\n",
    "\n",
    "for d_idx in sample_data_idx:\n",
    "######## 특정 샘플의 패턴 리스트 생성  \n",
    "    # sample_data_idx의 전체 패턴\n",
    "    specific_data_pattern_idx=[i for i,x in enumerate(consecutive_pattern) if x['data_idx'] ==d_idx]\n",
    "    specific_pattern_info = [consecutive_pattern[x] for x in specific_data_pattern_idx]\n",
    "    \n",
    "    #해당 데이터에 미리 정한 길이만큼의 패턴이 없을 경우 pass\n",
    "    if len(specific_pattern_info):\n",
    "        pass\n",
    "        \n",
    "    # 총 10개의 plot을 그릴때까지 진행\n",
    "    if plot_idx ==5:\n",
    "        break\n",
    "    \n",
    "    for p_channel in np.unique([(x['pattern_channel']) for x in specific_pattern_info if (x['data_idx']==d_idx)]):\n",
    "        plot_idx+=1\n",
    "        fig, ax= plt.subplots(figsize=(15,3))\n",
    "        ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "        spotlight = last_conv_bool[d_idx,:,p_channel]\n",
    "        \n",
    "        # 데이터 자체 그리기\n",
    "        data = trainX[d_idx,0,:,input_channel]\n",
    "        ax.plot(data,color='black')\n",
    "        ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "            \n",
    "        for spotlight_se in [(list(group)) for group in mit.consecutive_groups([i for i,x in enumerate(spotlight) if x==True])]:\n",
    "            ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "                \n",
    "        for idx,pattern_x, pattern in [(i,x['pattern_idx'],x['pattern']) for i,x in enumerate(specific_pattern_info) if (x['data_idx']==d_idx  and x['pattern_channel']== p_channel)]:\n",
    "            # 데이터의 해당 패턴 그리기\n",
    "            pattern_idx = specific_data_pattern_idx[idx]\n",
    "            color_idx = gak_km_predict[specific_data_pattern_idx.index(pattern_idx)]\n",
    "            pattern_x = consecutive_pattern[pattern_idx]['pattern_idx']\n",
    "            #pattern = consecutive_pattern[pattern_idx]['pattern']\n",
    "            ax.plot(pattern_x, data[pattern_x], color = color[color_idx])\n",
    "        fig.suptitle(\"prediction{}, data{}, output_channel{}\".format(np.argmax(prediction[d_idx]),d_idx,consecutive_pattern[pattern_idx]['pattern_channel']))\n",
    "        break\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with pseudoF num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = [\"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\",\n",
    "        \"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# turn on/off unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_off_acc_list=[original_acc]\n",
    "threshold = 0\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "for T in range(995, 800, -5):\n",
    "    \n",
    "    temp_threshold = np.percentile(last_conv_output,T/10.0)\n",
    "    temp_last_conv_bool = last_conv_output>temp_threshold\n",
    "    threshold_weight = (~temp_last_conv_bool).reshape(-1,1,128,128)\n",
    "    \n",
    "    turnoff_acc =MC_dropout_model.sess.run((MC_dropout_model.accuracy), \n",
    "                          feed_dict={MC_dropout_model.x: trainX,\n",
    "                                     MC_dropout_model.y: trainY,\n",
    "                                    MC_dropout_model.keep_prob:1.0,\n",
    "                                    MC_dropout_model.is_dropout:False,\n",
    "                                    MC_dropout_model.threshold_weight :threshold_weight})\n",
    "    \n",
    "    #print (\"[top {}% , threshold: {}]\\noriginal acc:{}, turnoff_acc{}\".format(100- T/10.0,threshold,original_acc,turnoff_acc))\n",
    "    acc_difference = original_acc - turnoff_acc\n",
    "    turn_off_acc_list.append(turnoff_acc)\n",
    "    if ((acc_difference > A)&(threshold==0)):\n",
    "        threshold = temp_threshold\n",
    "        last_conv_bool = temp_last_conv_bool\n",
    "        ax.scatter(100-(T/10.0),turnoff_acc,s=40,c='green')\n",
    "\n",
    "ax.scatter(0,original_acc,c='r',s=40)\n",
    "ax.plot([100- (x/10.0) for x in range(995+5, 800, -5)],turn_off_acc_list,c='black')\n",
    "ax.axhline(original_acc, c='r')\n",
    "ax.set_xlabel(\"TOP - %\")\n",
    "ax.set_ylabel(\"ACCURACY\")\n",
    "\n",
    "ax.set_xlim(0,100- (T/10.0))\n",
    "fig.suptitle(\"ACCURACY WHEN TURN OFF TOP-IMPORTANT NODE IN UNIT \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Activated output with threshold\n",
    "세로축(데이터) , 가로축(마지막 컨볼루션 채널) : 데이터와 별개로 각 채널마다 일관성있게 threshold를 넘은 구간이 존재함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(last_conv_bool.sum(axis=1),aspect =\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_channel = [i for i, x in enumerate(last_conv_bool.sum(axis=1).sum(axis=0)) if x>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top unit 끄면서 accuracy 변화보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_IOU(conv1,conv2_list, previous_mean,num,threshold):\n",
    "    conv1_bool = conv1>threshold\n",
    "    conv2_list_bool = conv2_list>threshold\n",
    "    IOU= [(x==conv1_bool).sum()/len(conv1) for x in conv2_list_bool]\n",
    "    IOU_mean = [previous+((new-previous)/num) for previous,new in zip(previous_mean,IOU)]\n",
    "\n",
    "    return IOU_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1003)\n",
    "random_data_idx = random.sample(range(conv_output2.shape[0]),500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_IOU = np.zeros(conv_output1.shape[2]*conv_output1.shape[2]).reshape(conv_output1.shape[2],conv_output1.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conv_c1 in range(conv_output1.shape[2]):\n",
    "    for num, d_idx in enumerate(random_data_idx):\n",
    "        conv1_IOU[conv_c1][:]= conv_IOU(conv_output1[d_idx,:,conv_c1],\n",
    "                                        conv_output1[d_idx,:,:].T,\n",
    "                                        conv1_IOU[conv_c1][:],\n",
    "                                        num+1,\n",
    "                                        threshold1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "high_cov=[]\n",
    "for conv_c1 in range(conv_output1.shape[2]):\n",
    "    high_cov_channel = [i for i,x in enumerate (conv1_IOU[conv_c1]>0.98) if (x == True and i!=conv_c1)]\n",
    "    high_cov.append(high_cov_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv1_IOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_IOU = np.zeros(conv_output2.shape[2]*conv_output2.shape[2]).reshape(conv_output2.shape[2],conv_output2.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conv_c1 in range(conv_output2.shape[2]):\n",
    "    for num, d_idx in enumerate(random_data_idx):\n",
    "        conv2_IOU[conv_c1][:]= conv_IOU(conv_output2[d_idx,:,conv_c1],\n",
    "                                        conv_output2[d_idx,:,:].T,\n",
    "                                        conv2_IOU[conv_c1][:],\n",
    "                                        num+1,\n",
    "                                        threshold2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv2_IOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3_IOU = np.zeros(conv_output3.shape[2]*conv_output3.shape[2]).reshape(conv_output3.shape[2],conv_output3.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conv_c1 in range(conv_output3.shape[2]):\n",
    "    for num, d_idx in enumerate(random_data_idx):\n",
    "        conv3_IOU[conv_c1][:]= conv_IOU(conv_output3[d_idx,:,conv_c1],\n",
    "                                        conv_output3[d_idx,:,:].T,\n",
    "                                        conv3_IOU[conv_c1][:],\n",
    "                                        num+1,\n",
    "                                        threshold3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv3_IOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_bool=np.ones(trainX.shape[0]*128*128).reshape(-1,1,128,128)\n",
    "false_bool = np.zeros(trainX.shape[0]*128*128).reshape(-1,1,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "each_turnoff_change=[original_acc]\n",
    "for i in range(128):\n",
    "    unit_bool =copy.deepcopy(true_bool)\n",
    "    unit_bool[:,:,:,i] = False\n",
    "    \n",
    "    acc =MC_dropout_model.sess.run((MC_dropout_model.accuracy), \n",
    "                          feed_dict={MC_dropout_model.x: trainX,\n",
    "                                     MC_dropout_model.y: trainY,\n",
    "                                    MC_dropout_model.keep_prob:1.0,\n",
    "                                    MC_dropout_model.is_dropout:False,\n",
    "                                    MC_dropout_model.threshold_weight :unit_bool})\n",
    "    each_turnoff_change.append(acc)\n",
    "    \n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(1,1,1,)\n",
    "ax.scatter(0,original_acc,c='r',s=40)\n",
    "ax.plot(each_turnoff_change,c='black')\n",
    "ax.axhline(original_acc, c='r')\n",
    "ax.set_xlabel(\"UNIT ID\")\n",
    "ax.set_ylabel(\"ACCURACY\")\n",
    "fig.suptitle(\"ACCURACY WHEN EACH UNIT IS TURNED OFF \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_change_order=np.argsort((original_acc - each_turnoff_change))[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## turn off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_turnoff_change=[original_acc]\n",
    "unit_bool =copy.deepcopy(true_bool)\n",
    "for unit_idx in acc_change_order:\n",
    "    if unit_idx==0:\n",
    "        continue\n",
    "    unit_bool[:,:,:,unit_idx-1] = False\n",
    "\n",
    "    acc =MC_dropout_model.sess.run((MC_dropout_model.accuracy), \n",
    "                          feed_dict={MC_dropout_model.x: trainX,\n",
    "                                     MC_dropout_model.y: trainY,\n",
    "                                    MC_dropout_model.keep_prob:1.0,\n",
    "                                    MC_dropout_model.is_dropout:False,\n",
    "                                    MC_dropout_model.threshold_weight :unit_bool})\n",
    "    top_turnoff_change.append(acc)   \n",
    "    \n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "ax.scatter(0,original_acc,c='r',s=40)\n",
    "ax.axhline(original_acc, c='r')\n",
    "ax.plot(top_turnoff_change,c='black')\n",
    "\n",
    "ax.set_xlabel(\"NUMBER OF TURNED OFF TOP-UNIT\")\n",
    "ax.set_ylabel(\"ACCURACY\")\n",
    "fig.suptitle(\"ACCURACY WHEN SEVERAL TOP-UNITS ARE TURNED OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unimportant_turnoff_change=[original_acc]\n",
    "unit_bool =copy.deepcopy(true_bool)\n",
    "for unit_idx in acc_change_order[::-1]:\n",
    "    if unit_idx==0:\n",
    "        pass\n",
    "    else:\n",
    "        unit_bool[:,:,:,unit_idx-1] = False\n",
    "\n",
    "        acc =MC_dropout_model.sess.run((MC_dropout_model.accuracy), \n",
    "                              feed_dict={MC_dropout_model.x: trainX,\n",
    "                                         MC_dropout_model.y: trainY,\n",
    "                                        MC_dropout_model.keep_prob:1.0,\n",
    "                                        MC_dropout_model.is_dropout:False,\n",
    "                                        MC_dropout_model.threshold_weight :unit_bool})\n",
    "        unimportant_turnoff_change.append(acc)   \n",
    "        \n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "ax.scatter(0,original_acc,c='r',s=40)\n",
    "ax.axhline(original_acc, c='r')\n",
    "ax.plot(unimportant_turnoff_change,c='black')\n",
    "\n",
    "ax.set_xlabel(\"NUMBER OF TURNED OFF UNIMPORTANT-UNIT\")\n",
    "ax.set_ylabel(\"ACCURACY\")\n",
    "fig.suptitle(\"ACCURACY WHEN SEVERAL UNIMPORTANT-UNITS ARE TURNED OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_turnon_acc =[]\n",
    "\n",
    "acc =MC_dropout_model.sess.run((MC_dropout_model.accuracy), \n",
    "                  feed_dict={MC_dropout_model.x: trainX,\n",
    "                             MC_dropout_model.y: trainY,\n",
    "                            MC_dropout_model.keep_prob:1.0,\n",
    "                            MC_dropout_model.is_dropout:False,\n",
    "                            MC_dropout_model.threshold_weight :false_bool})\n",
    "top_turnon_acc.append(acc)\n",
    "\n",
    "unit_bool=copy.deepcopy(false_bool)\n",
    "for unit_idx in acc_change_order:\n",
    "    if unit_idx==0:\n",
    "        pass\n",
    "    else:\n",
    "        unit_bool[:,:,:,unit_idx-1] = True\n",
    "        acc =MC_dropout_model.sess.run((MC_dropout_model.accuracy), \n",
    "                          feed_dict={MC_dropout_model.x: trainX,\n",
    "                                     MC_dropout_model.y: trainY,\n",
    "                                    MC_dropout_model.keep_prob:1.0,\n",
    "                                    MC_dropout_model.is_dropout:False,\n",
    "                                    MC_dropout_model.threshold_weight :unit_bool})\n",
    "        top_turnon_acc.append(acc)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "ax.axhline(original_acc, c='r')\n",
    "ax.plot(top_turnon_acc,c='black')\n",
    "ax.set_xlabel(\"NUMBER OF TURNED ON TOP-UNIT\")\n",
    "ax.set_ylabel(\"ACCURACY\")\n",
    "fig.suptitle(\"ACCURACY WHEN ONLY SEVERAL TOP-UNITS ARE TURNED ON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unimportant_turnon_acc =[]\n",
    "unit_bool=copy.deepcopy(false_bool)\n",
    "for unit_idx in acc_change_order[::-1]:\n",
    "    if unit_idx==0:\n",
    "        continue\n",
    "    unit_bool[:,:,:,unit_idx-1] = True\n",
    "    acc =MC_dropout_model.sess.run((MC_dropout_model.accuracy), \n",
    "                      feed_dict={MC_dropout_model.x: trainX,\n",
    "                                 MC_dropout_model.y: trainY,\n",
    "                                MC_dropout_model.keep_prob:1.0,\n",
    "                                MC_dropout_model.is_dropout:False,\n",
    "                                MC_dropout_model.threshold_weight :unit_bool})\n",
    "    unimportant_turnon_acc.append(acc)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "ax.axhline(original_acc, c='r')\n",
    "ax.plot(unimportant_turnon_acc,c='black')\n",
    "ax.set_xlabel(\"NUMBER OF TURNED ON UNIMPORTANT-UNIT\")\n",
    "ax.set_ylabel(\"ACCURACY\")\n",
    "fig.suptitle(\"ACCURACY WHEN ONLY SEVERAL UNIMPORTANT-UNITS ARE TURNED ON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패턴의 연속성 pattern candidate 추출(전체길이의 10퍼센트 이상이 highlight 된 부분)\n",
    "## 현재 서버 메모리때문에 trainX 500개까지만 패턴 클러스터링 적용\n",
    "### 모든 데이터를 위해서는 수정필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_num =trainX.shape[2]//20\n",
    "consecutive_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pattern_channel = 3\n",
    "consecutive_pattern=[]\n",
    "for data_idx,data in enumerate(trainX[:500,:,:,[input_pattern_channel]]):\n",
    "    for output_channel_idx in survived_channel:\n",
    "        pattern_bool =pd.DataFrame(last_conv_bool[data_idx][:,output_channel_idx])\n",
    "        highlight_idx = [p for p, x in enumerate(last_conv_bool[data_idx][:,output_channel_idx]) if x]\n",
    "        if highlight_idx:\n",
    "            groups = []\n",
    "            for _, g in groupby(highlight_idx, key=lambda n, c=itertools.count(): n-next(c)):\n",
    "                groups.append(list(g))    # Store group iterator as a list\n",
    "            for g in groups:\n",
    "                if len(g)>=consecutive_num:\n",
    "                    \n",
    "                    # 같은 데이터의 같은 부분이 패턴으로 인식될 때, repetitive_flag가 TRUE로 변환\n",
    "                    repetitive_flag=False\n",
    "                    for x in consecutive_pattern:\n",
    "                        if (x['data_idx']==data_idx and x['pattern_idx'] == g):\n",
    "                            x['pattern_channel'].append(output_channel_idx)\n",
    "                            repetitive_flag=True\n",
    "                    if not repetitive_flag:\n",
    "                        consecutive_pattern.append({\"data_idx\":data_idx, \"pattern_idx\":g, \"pattern\":data[0,g].flatten(), \"pattern_channel\":[output_channel_idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([len(x['pattern_idx']) for x in consecutive_pattern ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 길이 이상의 패턴들을 window slicing 하여 자르는 방법을 이용하실 경우,\n",
    "밑에 코드를 활성화 하시고, 위에 코드를 주석 처리하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step_size=2\n",
    "input_pattern_channel = 3\n",
    "\n",
    "pattern_length = 6\n",
    "consecutive_pattern=[]\n",
    "for data_idx,data in enumerate(trainX[:500,:,:,[input_pattern_channel]]):\n",
    "    for output_channel_idx in survived_channel:\n",
    "        pattern_bool =pd.DataFrame(last_conv_bool[data_idx][:,output_channel_idx])\n",
    "        highlight_idx = [p for p, x in enumerate(last_conv_bool[data_idx][:,output_channel_idx]) if x]\n",
    "        if highlight_idx:\n",
    "            groups = []\n",
    "            for _, g in groupby(highlight_idx, key=lambda n, c=itertools.count(): n-next(c)):\n",
    "                groups.append(list(g))    # Store group iterator as a list\n",
    "            for g in groups:\n",
    "                if len(g)>=pattern_length:\n",
    "                    for start_idx in range(0,len(g)-pattern_length+1,step_size):\n",
    "                        short_pattern_idx = [g[x] for x in range(start_idx, start_idx+pattern_length)]\n",
    "                        # 같은 데이터의 같은 부분이 패턴으로 인식될 때, repetitive_flag가 TRUE로 변환\n",
    "                        repetitive_flag=False\n",
    "                        for x in consecutive_pattern:\n",
    "                            if (x['data_idx']==data_idx and x['pattern_idx'] == short_pattern_idx):\n",
    "                                x['pattern_channel'].append(output_channel_idx)\n",
    "                                repetitive_flag=True\n",
    "                        if not repetitive_flag:\n",
    "                            consecutive_pattern.append({\"data_idx\":data_idx, \"pattern_idx\":short_pattern_idx, \"pattern\":data[0,short_pattern_idx].flatten(), \"pattern_channel\":[output_channel_idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [(x['pattern']) for x in consecutive_pattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from distutils.version import LooseVersion\n",
    "from scipy.stats import norm\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pattern[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a 1D density example\n",
    "N = 8\n",
    "np.random.seed(1)\n",
    "X = pattern[100][:,np.newaxis]\n",
    "\n",
    "X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "colors = ['navy', 'cornflowerblue', 'darkorange']\n",
    "kernels = ['gaussian', 'tophat', 'epanechnikov']\n",
    "lw = 2\n",
    "\n",
    "for color, kernel in zip(colors, kernels):\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)\n",
    "    log_dens = kde.score_samples(X_plot)\n",
    "    ax.plot(X_plot[:, 0], np.exp(log_dens), color=color, lw=lw,\n",
    "            linestyle='-', label=\"kernel = '{0}'\".format(kernel))\n",
    "\n",
    "ax.text(6, 0.38, \"N={0} points\".format(N))\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), '+k')\n",
    "\n",
    "ax.set_xlim(-4, 9)\n",
    "ax.set_ylim(-0.02, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.score(pattern[0].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pattern[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = [\"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\",\n",
    "        \"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering with fixed length data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 패턴의 길이를 fix하고 해당 length의 패턴만을 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_len = 6\n",
    "specific_len_pattern_idx=[i for i,x in enumerate(consecutive_pattern) if len(x['pattern_idx']) ==pattern_len]\n",
    "specific_pattern=[x['pattern'] for i,x in enumerate(consecutive_pattern) if i in specific_len_pattern_idx]\n",
    "len(specific_len_pattern_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pattern standardization\n",
    "1. 평균0, 분산1 형태로 standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "normalized_pattern_candidate =TimeSeriesScalerMeanVariance().fit_transform(specific_pattern)\n",
    "normalized_pattern_candidate = normalized_pattern_candidate.squeeze()\n",
    "normalized_pattern_candidate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spectral clustering, gmm clustering,k-means,SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture,cluster\n",
    "from tslearn.clustering import TimeSeriesKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_cluster : interger\n",
    "#eigen_solver{None, ‘arpack’, ‘lobpcg’, or ‘amg’}\n",
    "#affinity: ‘nearest_neighbors’‘rbf’\n",
    "default_spectral = {'n_clusters':2,\n",
    "                    'eigen_solver': 'arpack',\n",
    "                    'affinity': \"rbf\"}\n",
    "\n",
    "#covariance_type{‘full’ (default), ‘tied’, ‘diag’, ‘spherical’}\n",
    "default_gmm = {'n_clusters':2, 'covariance_type': 'full'}\n",
    "default_kmeans = {'n_clusters':2,'verbose':False,'random_state':0}\n",
    "default_hac = {'method':'single','metric':'euclidean','cut_off_level':5,'criterion':'maxclust'}\n",
    "\n",
    "spectral_option = [\n",
    "    ({'n_clusters': 6})\n",
    "]\n",
    "\n",
    "gmm_option = [\n",
    "    ({'n_clusters': 6})\n",
    "    \n",
    "]\n",
    "\n",
    "kmeans_option = [\n",
    "    ({'n_clusters': 6})\n",
    "    \n",
    "]\n",
    "\n",
    "hac_option =[\n",
    "    ({'cut_off_level': 6})\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "color = [\"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\",\n",
    "        \"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\"]\n",
    "\n",
    "random.seed(300)\n",
    "for g_option,k_option in zip(gmm_option,kmeans_option):\n",
    "\n",
    "    g_params = default_gmm.copy()\n",
    "    g_params.update(g_option)\n",
    "    \n",
    "    k_params = default_kmeans.copy()\n",
    "    k_params.update(k_option)\n",
    "    \n",
    "\n",
    "    gmm =  mixture.GaussianMixture(\n",
    "        n_components=g_params['n_clusters'], \n",
    "        covariance_type=g_params['covariance_type'])\n",
    "    \n",
    "    kmeans = cluster.KMeans(\n",
    "        n_clusters=k_params['n_clusters'], \n",
    "        random_state=k_params['random_state'])\n",
    "\n",
    "    g_pred = gmm.fit_predict(normalized_pattern_candidate)\n",
    "    k_pred = kmeans.fit_predict(normalized_pattern_candidate)\n",
    "    \n",
    "    g_cluster_mean_pattern =[]\n",
    "    k_cluster_mean_pattern =[]\n",
    "\n",
    "    fig = plt.figure(figsize=(len(np.unique(g_pred))*3,7))\n",
    "    axes = fig.subplots(2, g_params['n_clusters'])\n",
    "    for class_ in range(0,g_params['n_clusters']):\n",
    "        g_class_idx = [i for i,x in enumerate(g_pred) if x ==class_]\n",
    "        k_class_idx = [i for i,x in enumerate(k_pred) if x ==class_]\n",
    "\n",
    "        try:\n",
    "            axes[0,class_].plot(normalized_pattern_candidate[random.sample(g_class_idx,30)].T, color =color[class_],alpha=0.6)\n",
    "            axes[1,class_].plot(normalized_pattern_candidate[random.sample(k_class_idx,30)].T, color =color[class_],alpha=0.6)\n",
    "\n",
    "        except:\n",
    "            axes[0,class_].plot(normalized_pattern_candidate[g_class_idx].T, color =color[class_],alpha=0.6)\n",
    "            axes[1,class_].plot(normalized_pattern_candidate[k_class_idx].T, color =color[class_],alpha=0.6)\n",
    "\n",
    "        g_cluster_mean = normalized_pattern_candidate[g_class_idx].mean(axis=0)\n",
    "        g_cluster_mean_pattern.append(g_cluster_mean)\n",
    "        k_cluster_mean = normalized_pattern_candidate[k_class_idx].mean(axis=0)\n",
    "        k_cluster_mean_pattern.append(k_cluster_mean)\n",
    "        axes[0,class_].plot(g_cluster_mean, color ='black',alpha=1,linewidth=5)\n",
    "        axes[1,class_].plot(k_cluster_mean, color ='black',alpha=1,linewidth=5)\n",
    "\n",
    "    #fig.suptitle(\"Spectral Clustering: {} \\nGMM: {}\".format(s_params,g_params),y=0.98)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import more_itertools as mit\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = MC_dropout_model.predict(data=trainX,keep_prob=1.0,is_dropout=False,threshold_weight = np.ones(trainX.shape[0]*128*128).reshape(-1,1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_data_idx = [i for i,x in enumerate(prediction) if np.argmax(x)==1]\n",
    "sample_data_idx = [i for i,x in enumerate(prediction) if np.argmax(x)==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_idx = 0\n",
    "\n",
    "for d_idx in sample_data_idx:\n",
    "######## 특정 샘플의 패턴 리스트 생성  \n",
    "    # sample_data_idx의 전체 패턴\n",
    "    specific_data_pattern_idx=[i for i,x in enumerate(consecutive_pattern) if x['data_idx'] ==d_idx]\n",
    "    specific_pattern_idx= list(set(specific_data_pattern_idx) & set(specific_len_pattern_idx))\n",
    "    specific_pattern_info = [consecutive_pattern[x] for x in specific_pattern_idx]\n",
    "    \n",
    "    #해당 데이터에 미리 정한 길이만큼의 패턴이 없을 경우 pass\n",
    "    if len(specific_pattern_info):\n",
    "        pass\n",
    "        \n",
    "    # 총 10개의 plot을 그릴때까지 진행\n",
    "    if plot_idx ==15:\n",
    "        break\n",
    "    \n",
    "    for p_idx,p_info in zip(specific_pattern_idx, specific_pattern_info):\n",
    "        plot_idx+=1\n",
    "        # 데이터 자체 그리기\n",
    "        fig, ax= plt.subplots(figsize=(15,3))\n",
    "        ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "        data = trainX[d_idx,0,:,input_pattern_channel]\n",
    "        ax.plot(data,color='black')\n",
    "        \n",
    "        highlight_bool = np.zeros(width).astype(bool)\n",
    "        for p_channel in p_info['pattern_channel']:\n",
    "            hightlight_bool = (highlight_bool | last_conv_bool[d_idx,:,p_channel])\n",
    "        \n",
    "        \n",
    "        ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "            \n",
    "        for spotlight_se in [(list(group)) for group in mit.consecutive_groups([i for i,x in enumerate(hightlight_bool) if x==True])]:\n",
    "            ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "\n",
    "        color_idx = g_pred[specific_len_pattern_idx.index(p_idx)]\n",
    "        pattern_idx = p_info['pattern_idx']\n",
    "        ax.plot(pattern_idx, data[pattern_idx], color = color[color_idx])\n",
    "\n",
    "        # 패턴의 평균값 굵고 투명하게 그리기\n",
    "        x = g_cluster_mean_pattern[color_idx].reshape(-1,1) #returns a numpy array\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(data[pattern_idx].min(), data[pattern_idx].max()))\n",
    "        pattern_scaled = min_max_scaler.fit_transform(x)\n",
    "        ax.plot(pattern_idx, pattern_scaled, color = color[color_idx], alpha=0.4,linewidth=10)\n",
    "        fig.suptitle(\"data{} : label{}, prediction{} \\n in last conv output_channel{}\".format(d_idx, np.argmax(trainY[d_idx]),np.argmax(prediction[d_idx]),p_info['pattern_channel']),y=1.0)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering with different length scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Alignmnet Kernel K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_len = 6\n",
    "specific_len_pattern_idx=[i for i,x in enumerate(consecutive_pattern)]\n",
    "specific_pattern=[x['pattern'] for i,x in enumerate(consecutive_pattern) if i in specific_len_pattern_idx]\n",
    "len(specific_len_pattern_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.clustering import GlobalAlignmentKernelKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#길이를 맞추기위해 nan으로 패딩\n",
    "X =to_time_series_dataset(specific_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gak_km = GlobalAlignmentKernelKMeans(n_clusters=n_clusters)\n",
    "gak_km_predict = gak_km.fit_predict(X)\n",
    "\n",
    "class_breakdown(gak_km_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(n_clusters):\n",
    "    idx = [i for i,x in enumerate(gak_km_predict) if x==n]\n",
    "    vars()[\"pattern_\"+str(n)] = [pattern[i] for i in idx ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "for n in range(n_clusters):\n",
    "    ax= plt.subplot(1,n_clusters,n+1)\n",
    "    for p in eval(\"pattern_\"+str(n)):\n",
    "        ax.plot(range(len(p)),p,color = color[n])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "for n in range(n_clusters):\n",
    "    ax= plt.subplot(1,n_clusters,n+1)\n",
    "    mindexes = [np.argmax(x) for x in eval(\"pattern_\"+str(n))]\n",
    "    vars()[\"norm_pattern_\"+str(n)] = [(x-min(x)) for x in eval(\"pattern_\"+str(n))]\n",
    "    for i,p in enumerate(eval(\"norm_pattern_\"+str(n))):\n",
    "        shifts = max(mindexes) - mindexes[i]\n",
    "        ax.plot(range(shifts,shifts+len(p)), p,color=color[n],alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find representative shape for each clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using U shapelet\n",
    "ref: Clustering Time Series using Unsupervised-Shapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "sLen = range(5,9)\n",
    "D=copy.deepcopy(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_hat=[]\n",
    "ts=D[50]\n",
    "for _ in range(10):\n",
    "    cnt=0 #count of candidate u-shapelets from ts\n",
    "    s_hat ={} #set of subsequences, initially empty\n",
    "    gap = []\n",
    "    dt =[]\n",
    "        \n",
    "    for sl in sLen:#each u-shapelet length\n",
    "        for i in np.arange(0, len(ts)-sl +1):#each subsequence from ts \n",
    "            s_hat[cnt] = ts[i:i+sl] #a subsequence of length sl\n",
    "            tup = compute_gap(s_hat[cnt],D,6)\n",
    "            gap.append(tup[0])\n",
    "            dt.append(tup[1])\n",
    "            cnt+=1\n",
    "    if cnt>0:\n",
    "        index1 = np.argmax(gap)\n",
    "        S_hat.append(s_hat[index1]) #find maximum gap score\n",
    "        dis = compute_distance(s_hat[index1],D)#add the u-shapelet with max gap score\n",
    "        try:\n",
    "            dA= [dis[i] for i,x in enumerate(dis<dt[index1]) if x==True] #points to the left of dt\n",
    "        except:\n",
    "            dA = [dis[i] for i,x in enumerate(dis) if x<dt[0]]\n",
    "\n",
    "        if len(dA) ==1:\n",
    "            print(\"dA length:1\")\n",
    "            break\n",
    "        else:\n",
    "            index2 = np.argmax(dis)\n",
    "            ts = D[index2]\n",
    "            msdA= np.mean(dA) +np.std(dA)\n",
    "            D_hat = [i for i,x in enumerate(dis<msdA) if x ==True]\n",
    "            idx = [x for x in np.arange(0,len(D)) if x not in D_hat]\n",
    "            new_D=[]\n",
    "            for d_idx in idx:\n",
    "                new_D.append(D[d_idx])\n",
    "            D=new_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def znorm(data):\n",
    "    return (data-min(data))/(max(data)-min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_d(data1,data2):\n",
    "    return np.linalg.norm(data1-data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gap(sub, data,cluster_num):\n",
    "    dis= compute_distance(sub,data)\n",
    "    dis = np.sort(dis)\n",
    "    maxGap = 0\n",
    "    dt = 0\n",
    "    for l in range(1, len(dis)-1):\n",
    "        d = (dis[l]+dis[l+1])/2\n",
    "\n",
    "        dA = dis[dis<d]\n",
    "        dB = dis[dis>d]\n",
    "        try:\n",
    "            r = len(dA)/len(dB)\n",
    "            if ((1/cluster_num < r) and (1-(1/cluster_num))):\n",
    "                mA = dA.mean()\n",
    "                mB = dB.mean()\n",
    "                sA = dA.std()\n",
    "                sB = dB.std()\n",
    "                gap = mB-sB-(mA+sA)\n",
    "                if gap > maxGap :\n",
    "                    #print(\"gap {}\".format(gap))\n",
    "                    maxGap =gap\n",
    "                    dt =d\n",
    "        except:\n",
    "            pass\n",
    "    return maxGap, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(sub, data):\n",
    "    dis=[10e3]*len(data)\n",
    "    sub = znorm(sub)\n",
    "    for i in range(0,len(data)):\n",
    "        ts = data[i]\n",
    "\n",
    "        for j in range(0, len(ts)-len(sub)):\n",
    "            z = znorm(ts[j:j+len(sub)])\n",
    "            d = euclidean_d(z,sub)\n",
    "            dis[i]= min(d,dis[i])/np.sqrt(len(sub))\n",
    "        #print(dis[i])\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use dtw measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtwalign import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = norm_pattern_5[60]\n",
    "y = norm_pattern_5[0]\n",
    "res = dtw(x,y)\n",
    "\n",
    "# dtw distance\n",
    "print(\"dtw distance: {}\".format(res.distance))\n",
    "print(\"dtw normalized distance: {}\".format(res.normalized_distance))\n",
    "\n",
    "\"\"\"\n",
    "if you want to calculate only dtw distance (i.e. no need to gain alignment path),\n",
    "give 'distance_only' argument as True (it makes faster).\n",
    "\"\"\"\n",
    "#res = dtw(x,y,distance_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = plt.subplot(2,1,1)\n",
    "ax.plot(x,color=\"blue\")\n",
    "ax.plot(y,color=\"red\")\n",
    "ax.set_ylim(0,1)\n",
    "# warp both x and y by alignment path\n",
    "x_path = res.path[:,0]\n",
    "y_path = res.path[:,1]\n",
    "\n",
    "ax = plt.subplot(2,1,2)\n",
    "ax.plot(x[x_path],label=\"aligned query\",color=\"blue\")\n",
    "ax.plot(y[y_path],label=\"aligned reference\",color=\"red\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize alignment path with cumsum cost matrix\n",
    "res.plot_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = MC_dropout_model.predict(data=trainX,keep_prob=1.0,is_dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_data_idx = [i for i,x in enumerate(prediction) if np.argmax(x)==1]\n",
    "sample_data_idx = [i for i,x in enumerate(prediction) if np.argmax(x)==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_idx = 0\n",
    "\n",
    "for d_idx in sample_data_idx:\n",
    "######## 특정 샘플의 패턴 리스트 생성  \n",
    "    # sample_data_idx의 전체 패턴\n",
    "    specific_data_pattern_idx=[i for i,x in enumerate(consecutive_pattern) if x['data_idx'] ==d_idx]\n",
    "    specific_pattern_info = [consecutive_pattern[x] for x in specific_data_pattern_idx]\n",
    "    \n",
    "    #해당 데이터에 미리 정한 길이만큼의 패턴이 없을 경우 pass\n",
    "    if len(specific_pattern_info):\n",
    "        pass\n",
    "        \n",
    "    # 총 10개의 plot을 그릴때까지 진행\n",
    "    if plot_idx ==5:\n",
    "        break\n",
    "    \n",
    "    for p_channel in np.unique([(x['pattern_channel']) for x in specific_pattern_info if (x['data_idx']==d_idx)]):\n",
    "        plot_idx+=1\n",
    "        fig, ax= plt.subplots(figsize=(15,3))\n",
    "        ax.set_xlim([0,int(trainX.shape[2])-1])\n",
    "        spotlight = last_conv_bool[d_idx,:,p_channel]\n",
    "        \n",
    "        # 데이터 자체 그리기\n",
    "        data = trainX[d_idx,0,:,input_channel]\n",
    "        ax.plot(data,color='black')\n",
    "        ax.set_facecolor(\"grey\") # 패턴이 있는 부분만 흰색 spotlight으로 하기 위해 기본 배경을 회색으로 설정\n",
    "            \n",
    "        for spotlight_se in [(list(group)) for group in mit.consecutive_groups([i for i,x in enumerate(spotlight) if x==True])]:\n",
    "            ax.axvspan(spotlight_se[0], spotlight_se[-1], color='white', alpha=1)\n",
    "                \n",
    "        for idx,pattern_x, pattern in [(i,x['pattern_idx'],x['pattern']) for i,x in enumerate(specific_pattern_info) if (x['data_idx']==d_idx  and x['pattern_channel']== p_channel)]:\n",
    "            # 데이터의 해당 패턴 그리기\n",
    "            pattern_idx = specific_data_pattern_idx[idx]\n",
    "            color_idx = gak_km_predict[specific_data_pattern_idx.index(pattern_idx)]\n",
    "            pattern_x = consecutive_pattern[pattern_idx]['pattern_idx']\n",
    "            #pattern = consecutive_pattern[pattern_idx]['pattern']\n",
    "            ax.plot(pattern_x, data[pattern_x], color = color[color_idx])\n",
    "        fig.suptitle(\"prediction{}, data{}, output_channel{}\".format(np.argmax(prediction[d_idx]),d_idx,consecutive_pattern[pattern_idx]['pattern_channel']))\n",
    "        break\n",
    "\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below part is still unclear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = open('file.txt', 'a+t')\n",
    "for i,x in enumerate(X):\n",
    "    for x_ in x:\n",
    "        f.write(str(x_[0])+\",\")\n",
    "    f.write(\":\"+str(gak_km_predict[i])+\"\\n\")\n",
    "\n",
    "f.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(columns=[\"dim_0\"],index=range(len(X)))\n",
    "for i,x in enumerate(X):\n",
    "    X_df.iloc[i,0] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df =gak_km_predict.astype('<U1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformers.shapelets import ContractedShapeletTransform\n",
    "# How long (in minutes) to extract shapelets for.\n",
    "# This is a simple lower-bound initially; once time is up, no further shapelets will be assessed\n",
    "time_limit_in_mins = 1\n",
    "\n",
    "# The initial number of shapelet candidates to assess per training series. If all series are visited\n",
    "# and time remains on the contract then another pass of the data will occur\n",
    "initial_num_shapelets_per_case = 5\n",
    "\n",
    "# Whether or not to print on-going information about shapelet extraction. Useful for demo/debugging\n",
    "verbose = 2\n",
    "\n",
    "st = ContractedShapeletTransform(\n",
    "    time_limit_in_mins=time_limit_in_mins,\n",
    "    num_candidates_to_sample_per_case=initial_num_shapelets_per_case,\n",
    "    verbose=verbose)\n",
    "st.fit(X_df, Y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for each extracted shapelet (in descending order of quality/information gain)\n",
    "for s in st.shapelets:\n",
    "\n",
    "    # summary info about the shapelet\n",
    "    print(s)\n",
    "\n",
    "    # plot the series that the shapelet was extracted from\n",
    "    plt.plot(\n",
    "        X[s.series_id],\n",
    "        'gray'\n",
    "    )\n",
    "\n",
    "    # overlay the shapelet onto the full series\n",
    "    plt.plot(\n",
    "        list(range(s.start_pos,(s.start_pos+s.length))),\n",
    "        X[s.series_id,s.start_pos:s.start_pos+s.length],\n",
    "        'r',\n",
    "        linewidth=3.0\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "D = hac.linkage(normalized_pattern_candidate[:20], method='single',metric='euclidean')\n",
    "cut_off_level = 5\n",
    "#Dendrogram\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Dendrogram of time series clustering',fontsize=25, fontweight='bold')\n",
    "plt.xlabel('sample index', fontsize=25, fontweight='bold')\n",
    "plt.ylabel('distance', fontsize=25, fontweight='bold')\n",
    "hac.dendrogram(D, leaf_rotation=90., leaf_font_size=15., ) # font size for the x axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 파라미터에 해당하는 클러스터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(300)\n",
    "sample_data_idx = random.sample(range(0,len(normalized_pattern_candidate)),len(normalized_pattern_candidate))\n",
    "color = [\"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\",\n",
    "        \"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_option,g_option,k_option in zip(spectral_option[::-1],gmm_option[::-1],kmeans_option[::-1]):\n",
    "    s_params = default_spectral.copy()\n",
    "    s_params.update(s_option)\n",
    "    \n",
    "    g_params = default_gmm.copy()\n",
    "    g_params.update(g_option)\n",
    "    \n",
    "    k_params = default_kmeans.copy()\n",
    "    k_params.update(k_option)\n",
    "    \n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=s_params['n_clusters'], \n",
    "        eigen_solver=s_params['eigen_solver'],\n",
    "        affinity=s_params['affinity'])\n",
    "    \n",
    "    gmm =  mixture.GaussianMixture(\n",
    "        n_components=g_params['n_clusters'], \n",
    "        covariance_type=g_params['covariance_type'])\n",
    "    \n",
    "    kmeans = cluster.KMeans(\n",
    "        n_clusters=k_params['n_clusters'], \n",
    "        random_state=k_params['random_state'])\n",
    "    \n",
    "    spectral.fit(normalized_pattern_candidate)\n",
    "    gmm.fit(normalized_pattern_candidate)\n",
    "    kmeans.fit(normalized_pattern_candidate)\n",
    "    \n",
    "    s_pred = spectral.labels_[sample_data_idx]\n",
    "    g_pred = gmm.predict(normalized_pattern_candidate[sample_data_idx])\n",
    "    k_pred = kmeans.fit_predict(normalized_pattern_candidate[sample_data_idx])\n",
    "    \n",
    "    num_class = len(list(set(np.unique(s_pred)).union(set(np.unique(g_pred)))))\n",
    "    fig = plt.figure(figsize=(18,5))\n",
    "    axes = fig.subplots(3,num_class)    \n",
    "    \n",
    "    for class_ in range(0,num_class):\n",
    "        class_idx = [i for i,x in enumerate(s_pred) if x ==class_]\n",
    "        try:\n",
    "            axes[0][class_+1].plot(normalized_pattern_candidate[[sample_data_idx[x] for x in class_idx]].T, color =color[class_],alpha=0.6)\n",
    "            axes[0][0].plot(normalized_pattern_candidate[[sample_data_idx[x] for x in class_idx]].mean(axis=0), color = 'black',alpha=1,linewidth=3,\n",
    "                   path_effects=[pe.Stroke(linewidth=10, foreground=color[class_]), pe.Normal()])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        class_idx = [i for i,x in enumerate(g_pred) if x ==class_]\n",
    "        try:\n",
    "            axes[1][class_+1].plot(normalized_pattern_candidate[[sample_data_idx[x] for x in class_idx]].T, color =color[class_],alpha=0.6)\n",
    "            axes[1][0].plot(normalized_pattern_candidate[[sample_data_idx[x] for x in class_idx]].mean(axis=0), color ='black',alpha=1,linewidth=3,\n",
    "                    path_effects=[pe.Stroke(linewidth=10, foreground=color[class_]), pe.Normal()])\n",
    "        except:\n",
    "            pass\n",
    "        class_idx = [i for i,x in enumerate(k_pred) if x ==class_]\n",
    "        try:\n",
    "            axes[2][class_+1].plot(normalized_pattern_candidate[[sample_data_idx[x] for x in class_idx]].T, color =color[class_],alpha=0.6)\n",
    "            axes[2][0].plot(normalized_pattern_candidate[[sample_data_idx[x] for x in class_idx]].mean(axis=0), color ='black',alpha=1,linewidth=3,\n",
    "                    path_effects=[pe.Stroke(linewidth=10, foreground=color[class_]), pe.Normal()])\n",
    "        except:\n",
    "            pass\n",
    "    #fig.suptitle(\"Spectral Clustering: {} \\nGMM: {}\".format(s_params,g_params),y=0.98)\n",
    "    plt.tight_layout()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sompy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = sompy.SOMFactory.build(normalized_pattern_candidate, mapsize=None, normalization='var', initialization='pca', neighborhood='gaussian', training='batch', name='sompy')  # this will use the default parameters, but i can change the initialization and neighborhood methods\n",
    "som.train(n_job=1, verbose=False)  # verbose='debug' will print more, and verbose=None wont print anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.visualization.hitmap import HitMapView\n",
    "som.cluster(4)\n",
    "hits  = HitMapView(20,20,\"Clustering\",text_size=12)\n",
    "a=hits.show(som)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.visualization.mapview import View2D\n",
    "view2D  = View2D(10,10,\"rand data\",text_size=10)\n",
    "view2D.show(som, col_sz=4, which_dim=\"all\", desnormalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.visualization.bmuhits import BmuHitsView\n",
    "\n",
    "vhts  = BmuHitsView(4,4,\"Hits Map\",text_size=12)\n",
    "vhts.show(som, anotate=True, onlyzeros=False, labelsize=12, cmap=\"Greys\", logaritmic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## represent as feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import do_wavelet\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = normalized_pattern_candidate[3].shape[0]\n",
    "t0=10\n",
    "dt=1\n",
    "time = np.arange(0, N) * dt + t0\n",
    "signal = normalized_pattern_candidate[3]\n",
    " \n",
    "scales = np.arange(1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "spec = gridspec.GridSpec(ncols=6, nrows=6)\n",
    "top_ax = fig.add_subplot(spec[:2, 0:5])\n",
    "bottom_left_ax = fig.add_subplot(spec[2:, 0:5])\n",
    "bottom_right_ax = fig.add_subplot(spec[2:, 5])\n",
    "\n",
    "plot_signal_plus_average(top_ax, time, signal, average_over = 3)\n",
    "yticks, ylim = plot_wavelet(bottom_left_ax, time, signal, scales, xlabel=xlabel, ylabel=ylabel, title=title)\n",
    "\n",
    "plot_fft_plus_power(bottom_right_ax, time, signal, plot_direction='vertical', yticks=yticks, ylim=ylim)\n",
    "bottom_right_ax.set_ylabel('Period [years]', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. clustering time series using unsupervised shapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D =pattern.copy(deep=True)\n",
    "sLen = range(6,32)\n",
    "\n",
    "S_hat={}\n",
    "ts =D[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_option =spectral_option[-1]\n",
    "g_option=gmm_option[-1]\n",
    "s_params = default_spectral.copy()\n",
    "s_params.update(s_option)\n",
    "\n",
    "g_params = default_gmm.copy()\n",
    "g_params.update(g_option)\n",
    "\n",
    "spectral = cluster.SpectralClustering(\n",
    "    n_clusters=s_params['n_clusters'], \n",
    "    eigen_solver=s_params['eigen_solver'],\n",
    "    affinity=s_params['affinity'])\n",
    "\n",
    "gmm =  mixture.GaussianMixture(\n",
    "    n_components=g_params['n_clusters'], \n",
    "    covariance_type=g_params['covariance_type'])\n",
    "\n",
    "s_pred = spectral.fit_predict(normalized_pattern_candidate)\n",
    "g_pred = gmm.fit_predict(normalized_pattern_candidate)\n",
    "\n",
    "s_representative_pattern =[]\n",
    "g_representative_pattern =[]\n",
    "for class_ in np.unique(s_pred):\n",
    "\n",
    "    s_class_idx = [i for i,x in enumerate(s_pred) if x ==class_]\n",
    "    s_representative_pattern.append(normalized_pattern_candidate[s_class_idx].mean(axis=0))\n",
    "\n",
    "    g_class_idx = [i for i,x in enumerate(g_pred) if x ==class_]\n",
    "    g_representative_pattern.append(normalized_pattern_candidate[g_class_idx].mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "ax = fig.add_subplot(1, 2, 1,projection='3d')\n",
    "\n",
    "x = ((g_representative_pattern[0]*len(normalized_pattern_candidate)-normalized_pattern_candidate)**2).sum(axis=1)\n",
    "y = ((g_representative_pattern[1]*len(normalized_pattern_candidate)-normalized_pattern_candidate)**2).sum(axis=1)\n",
    "z = ((g_representative_pattern[2]*len(normalized_pattern_candidate)-normalized_pattern_candidate)**2).sum(axis=1)\n",
    "\n",
    "ax.scatter(x,y,z, c = g_pred)\n",
    "\n",
    "ax.set_xlabel('class1')\n",
    "ax.set_ylabel('class2')\n",
    "ax.set_zlabel('class3')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "#ax.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2,projection='3d')\n",
    "\n",
    "x = ((s_representative_pattern[0]*len(normalized_pattern_candidate)-normalized_pattern_candidate)**2).sum(axis=1)\n",
    "y = ((s_representative_pattern[1]*len(normalized_pattern_candidate)-normalized_pattern_candidate)**2).sum(axis=1)\n",
    "z = ((s_representative_pattern[2]*len(normalized_pattern_candidate)-normalized_pattern_candidate)**2).sum(axis=1)\n",
    "\n",
    "ax2.scatter(x,y,z, c = s_pred)\n",
    "\n",
    "ax2.set_xlabel('class1')\n",
    "ax2.set_ylabel('class2')\n",
    "ax2.set_zlabel('class3')\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_zticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "color = [\"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\",\n",
    "        \"crimson\",\"orange\",\"olive\",'seagreen',\"royalblue\",\"purple\",\"pink\",\"grey\"]\n",
    "random.seed(300)\n",
    "for s_option,g_option in zip(spectral_option,gmm_option):\n",
    "    s_params = default_spectral.copy()\n",
    "    s_params.update(s_option)\n",
    "    \n",
    "    g_params = default_gmm.copy()\n",
    "    g_params.update(g_option)\n",
    "    \n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=s_params['n_clusters'], \n",
    "        eigen_solver=s_params['eigen_solver'],\n",
    "        affinity=s_params['affinity'])\n",
    "    \n",
    "    gmm =  mixture.GaussianMixture(\n",
    "        n_components=g_params['n_clusters'], \n",
    "        covariance_type=g_params['covariance_type'])\n",
    "    \n",
    "    s_pred = spectral.fit(normalized_pattern_candidate)\n",
    "    g_pred = gmm.fit(normalized_pattern_candidate)\n",
    "    \n",
    "    sample_data_idx = random.sample(range(0,len(normalized_pattern_candidate)),100)\n",
    "    \n",
    "    s_pred = spectral.labels_[sample_data_idx]\n",
    "    g_pred = gmm.predict(normalized_pattern_candidate[sample_data_idx])\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,3))\n",
    "    for class_ in np.unique(s_pred):\n",
    "        ax = plt.subplot(1,2,1)\n",
    "        class_idx = [sample_data_idx[i] for i,x in enumerate(s_pred) if x ==class_]\n",
    "        ax.plot(normalized_pattern_candidate[class_idx].T, color =color[class_],alpha=0.2)\n",
    "        ax.plot(normalized_pattern_candidate[class_idx].mean(axis=0), color =color[class_],alpha=1,linewidth=5)\n",
    "\n",
    "        ax2 = plt.subplot(1,2,2)\n",
    "        class_idx = [sample_data_idx[i] for i,x in enumerate(g_pred) if x ==class_]\n",
    "        try:\n",
    "            #ax2.plot(normalized_pattern_candidate[class_idx].T, color =color[class_],alpha=0.2)\n",
    "            ax2.plot(normalized_pattern_candidate[class_idx].mean(axis=0), color =color[class_],alpha=1,linewidth=5)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    fig.suptitle(\"Spectral Clustering: {} \\nGMM: {}\".format(s_params,g_params))\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotly visualization을 위해 샘플 데이터 및 패턴 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_idx=87\n",
    "sample_output_channel = 79\n",
    "sample_data = pd.DataFrame(trainX[sample_data_idx].reshape(-1,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pattern_bool =pd.DataFrame(last_conv_bool[sample_data_idx][:,sample_output_channel])\n",
    "pattern_idx = [p for p, x in enumerate(last_conv_bool[sample_data_idx][:,sample_output_channel]) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_bool[sample_data_idx][:,sample_output_channel].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "for _, g in groupby(pattern_idx, key=lambda n, c=itertools.count(): n-next(c)):\n",
    "    groups.append(list(g))    # Store group iterator as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_pattern=[]\n",
    "for g in groups:\n",
    "    if len(g)>consecutive_num:\n",
    "        consecutive_pattern.extend(g)\n",
    "        \n",
    "consecutive_pattern_bool=[]\n",
    "for x in range(0,128):\n",
    "    if x in np.asarray(consecutive_pattern).flatten():\n",
    "        consecutive_pattern_bool.append(True)\n",
    "    else:\n",
    "        consecutive_pattern_bool.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_consecutive_pattern_bool= pd.DataFrame(consecutive_pattern_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_data.to_csv('./sample_data+pattern/'+str(sample_data_idx)+'_data.csv')\n",
    "sample_pattern_bool.to_csv('./sample_data+pattern/'+str(sample_data_idx)+'_pattern.csv')\n",
    "sample_consecutive_pattern_bool.to_csv('./sample_data+pattern/'+str(sample_data_idx)+'_consecutive_pattern.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "DEFAULT_PLOTLY_COLORS=['rgba(31, 119, 180, {})', 'rgba(255, 127, 14, {})',\n",
    "                       'rgba(44, 160, 44, {})', 'rgba(214, 39, 40, {})',\n",
    "                       'rgba(148, 103, 189, {})', 'rgba(140, 86, 75, {})',\n",
    "                       'rgba(227, 119, 194, {})', 'rgba(127, 127, 127, {})',\n",
    "                       'rgba(188, 189, 34, {})', 'rgba(23, 190, 207, {})']\n",
    "\n",
    "normalized_data = (sample_data.values - sample_data.values.min(axis=0))/(sample_data.values.max(axis=0) - sample_data.values.min(axis=0))\n",
    "for cidx in range(0,sample_data.shape[1]):\n",
    "    normalized_data[:,cidx] =normalized_data[:,cidx] + cidx \n",
    "    \n",
    "    \n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1, shared_xaxes=True, vertical_spacing=0.02\n",
    ")\n",
    "\n",
    "for cidx in range(sample_data.shape[-1]):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(sample_data.shape[0]), \n",
    "                             y=normalized_data[:,cidx],  \n",
    "                            line_color=\"black\",\n",
    "                             line_width=1,\n",
    "                             showlegend=False,\n",
    "                             ),\n",
    "                  row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=np.arange(sample_data.shape[0])[consecutive_pattern_bool], \n",
    "                            y=normalized_data[consecutive_pattern_bool,cidx],  \n",
    "#                               fill='toself',\n",
    "                            line_color='rgba(200, 0, 200, 0.6)',\n",
    "                            line_width=5,\n",
    "                            showlegend=False,\n",
    "                            ),\n",
    "                   row=1, col=1)\n",
    "    \n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black',ticks=\"inside\")\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black',\n",
    "                 ticks=\"inside\",nticks=sample_data.shape[1], tickwidth=2, tickcolor='black', ticklen=10,title_text='Input Channel',\n",
    "                showgrid=True, gridwidth=1, gridcolor='grey',\n",
    "                ticktext=[\"Channel1\", \"Channel2\", \"Channel3\", \"Channel4\",\"Channel5\", \"Channel6\", \"Channel7\", \"Channel8\", \"Channel9\"],\n",
    "                tickvals=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\",\"7\", \"8\", \"9\"],\n",
    "                range=[0,9])\n",
    "\n",
    "\n",
    "fig.update_layout(height=600, width=800,plot_bgcolor='rgb(256,256,256)',\n",
    "                  title_text=\"Input\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 10005\n",
    "\n",
    "class_color= [\"cyan\",\"orange\",\"lime\",\"pink\",\"cyan\",\"orange\",\"lime\",\"pink\",\"cyan\"]\n",
    "consecutive_class_color= [\"dodgerblue\",\"coral\",\"seagreen\",\"indianred\",\"dodgerblue\",\"coral\",\"seagreen\",\"indianred\",\"dodgerblue\"]\n",
    "survived_channel = [i for i, x in enumerate(last_conv_bool.sum(axis=1).sum(axis=0)) if x>0]\n",
    "sample_channel = random.sample(survived_channel,1)\n",
    "\n",
    "consecutive_num =trainX.shape[2]//20\n",
    "\n",
    "for class_i in class_list:\n",
    "    \n",
    "    sample_data_idx =random.sample([i for i, e in enumerate(np.argmax(trainY,axis=1)) if e == (class_i-1)],1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111,projection='3d')\n",
    "    fig.suptitle(\"Last Conv ({}) Consecutive ({}) ,Output(class{}) \".format(sample_channel,consecutive_num,class_i-1))\n",
    "    \n",
    "    for c in range(0,input_channel):\n",
    "        data = trainX[sample_data_idx,:,:,c].reshape(-1,1)\n",
    "        data = (data-data.min())/(data.max()-data.min())\n",
    "        pattern_idx = [p for p, x in enumerate(last_conv_bool[sample_data_idx[0],:,c]) if x]\n",
    "        #ax = fig.gca(projection='3d')\n",
    "\n",
    "        ax.plot([c]*len(data),range(0,len(data)), data.flatten())\n",
    "        # 패턴의 연속성을 찾는 구간\n",
    "        groups = []\n",
    "        for _, g in groupby(pattern_idx, key=lambda n, c=itertools.count(): n-next(c)):\n",
    "            groups.append(list(g))    # Store group iterator as a list\n",
    "        for g in groups:\n",
    "            if len(g)>consecutive_num:\n",
    "                ax.scatter([c]*len(g) ,g,data[g],color = consecutive_class_color[c], s =12,alpha=0.7)\n",
    "                #ax.add_collection3d(plt.fill_between(np.arange(g[0],g[-1]),data[g[0]:g[-1]],0), zs=1, zdir='y')\n",
    "                #ax[j].text(g[0],0,len(g))\n",
    "            #else:\n",
    "                #ax.scatter(g,[c]*len(g), data[g],color = class_color[c],s=12)\n",
    "                \n",
    "        ax.set_xticks=[]\n",
    "        ax.set_yticks=[]\n",
    "        #ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([1, 0.8, 0.2, 1]))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
